{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"manylatents","text":"<p>Dimensionality reduction and neural network analysis. Built on PyTorch Lightning and Hydra.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/latent-reasoning-works/manylatents.git\ncd manylatents &amp;&amp; uv sync\n\nuv run python -m manylatents.main data=swissroll algorithms/latent=pca\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Algorithms \u2014 LatentModule (fit/transform) and LightningModule (trainable) algorithms, networks, and losses</li> <li>Data \u2014 Synthetic manifolds, precomputed data, and sampling strategies</li> <li>Metrics \u2014 Three-level evaluation system: embedding, dataset, and module metrics</li> <li>Evaluation \u2014 Algorithm dispatch, sampling strategies, and shared caching</li> <li>Cache Protocol \u2014 Shared kNN cache, config sleuther, metric expansion</li> <li>Callbacks \u2014 Embedding callbacks (save, plot, wandb) and trainer callbacks (probing)</li> <li>Extensions \u2014 Install, use, and develop domain extensions</li> <li>API \u2014 Programmatic API for agent-driven multi-step workflows</li> <li>Probing \u2014 Representation probing for auditing algorithm internals during training</li> <li>Testing \u2014 CI pipeline, namespace integration testing, and mock package patterns</li> </ul>"},{"location":"TODO/","title":"ManyLatents TODO","text":""},{"location":"TODO/#pre-release-checklist","title":"Pre-Release Checklist","text":""},{"location":"TODO/#add-lightning-module-cli-test","title":"Add Lightning Module CLI Test","text":"<p>Priority: High (blocking release) Status: COMPLETED</p> <p>Add a CLI test for Lightning modules (e.g., autoencoder) to validate training works.</p> <p>Implementation: <pre><code># .github/workflows/build.yml\n- name: Test CLI - Lightning Module\n  run: |\n    source .venv/bin/activate\n    uv run python -m manylatents.main \\\n      algorithms/lightning=ae_reconstruction \\\n      data=swissroll \\\n      metrics=noop \\\n      callbacks/embedding=minimal \\\n      trainer.max_epochs=2 \\\n      trainer.fast_dev_run=true\n</code></pre></p> <p>Why important: - Validates Lightning training loop works - Tests neural network path (not just LatentModule) - Ensures <code>fast_dev_run</code> flag works correctly</p>"},{"location":"TODO/#testing-ci","title":"Testing &amp; CI","text":""},{"location":"TODO/#module-instantiation-sweep-test","title":"Module Instantiation Sweep Test","text":"<p>Priority: High Status: COMPLETED</p> <p>Add pytest that sweeps through all algorithm modules and validates they can be instantiated.</p> <p>Goal: Ensure all modules in <code>manylatents/algorithms/</code> can be imported and initialized without errors.</p> <p>Implementation: <pre><code># tests/test_module_instantiation.py\nimport pytest\nfrom pathlib import Path\nimport importlib\n\ndef test_all_algorithms_can_instantiate():\n    \"\"\"Test that all algorithm modules can be imported and instantiated.\"\"\"\n\n    # Collect all Python files in algorithms/\n    algo_dir = Path(\"manylatents/algorithms\")\n\n    for module_file in algo_dir.rglob(\"*.py\"):\n        if module_file.name.startswith(\"_\") or module_file.name.startswith(\"test_\"):\n            continue\n\n        # Convert path to module name\n        module_path = str(module_file.relative_to(\".\")).replace(\"/\", \".\")[:-3]\n\n        # Import and check for classes\n        module = importlib.import_module(module_path)\n\n        # Find classes that inherit from LatentModule or LightningModule\n        # Try to instantiate with minimal config\n        # Assert no import errors or instantiation failures\n</code></pre></p> <p>Benefits: - Catches import errors early - Validates all modules are properly structured - Prevents broken algorithms from being merged - Fast smoke test for entire algorithm library</p>"},{"location":"TODO/#architecture","title":"Architecture","text":""},{"location":"TODO/#full-non-lightning-inference-mode","title":"Full Non-Lightning Inference Mode","text":"<p>Priority: Low (future consideration) Status: Not started</p> <p>Currently, <code>FoundationEncoder</code> (from <code>manylatents-omics</code>) uses a workaround: it inherits from <code>LatentModule</code> with <code>fit()</code> as no-op and <code>transform()</code> calling <code>datamodule.get_sequences()</code>. This works but has limitations:</p> <p>Current workaround (<code>manylatents/algorithms/encoder/base.py</code>): <pre><code>class FoundationEncoder(LatentModule):\n    def fit(self, x: Tensor) -&gt; None:\n        self._is_fitted = True  # no-op\n\n    def transform(self, x: Tensor) -&gt; Tensor:\n        sequences = self.datamodule.get_sequences()  # ignores x\n        return self.encode_batch(sequences)\n</code></pre></p> <p>Limitations: - The <code>x</code> tensor parameter is ignored (sequences come from datamodule) - <code>experiment.py</code> requires special-casing to skip tensor unrolling for FoundationEncoder - Doesn't fit cleanly into the fit/transform paradigm (nothing to fit)</p> <p>Potential solution: Add a dedicated <code>inference</code> algorithm mode alongside <code>latent</code> and <code>lightning</code>: - No fit step, just encode - Direct datamodule \u2192 encoder \u2192 embeddings pipeline - Cleaner separation from dimensionality reduction algorithms</p> <p>References: - Extension implementation: <code>manylatents-omics/manylatents/dogma/encoders/</code> - SequenceDataModule: <code>manylatents-omics/manylatents/dogma/data/sequence_dataset.py</code> - Experiment special-casing: <code>manylatents/experiment.py</code> (FoundationEncoder branch)</p>"},{"location":"TODO/#completed","title":"Completed","text":""},{"location":"TODO/#logging-config-group","title":"Logging Config Group","text":"<p>Priority: Medium Status: \u2705 COMPLETED</p> <p>Implemented <code>logger</code> config group to control wandb initialization: - <code>logger=none</code> - No wandb, fastest (default for CI) - <code>logger=wandb</code> - Full wandb integration</p>"},{"location":"TODO/#centralized-tensor-conversion","title":"Centralized Tensor Conversion","text":"<p>Priority: Medium Status: \u2705 COMPLETED</p> <p>Moved tensor-to-numpy conversion from individual metrics to <code>evaluate_embeddings()</code> in <code>experiment.py</code>. Removed duplicate code from 10+ metric files.</p>"},{"location":"algorithms/","title":"Algorithms","text":"<p>manyLatents provides two algorithm base classes. The decision rule is binary: if the algorithm trains with backprop, use LightningModule. If not, use LatentModule.</p> LatentModule <p>|---|---|---|---| | ArchetypalAnalysisModule | <code>latent</code> | <code>algorithms/latent=aa</code> | <code>method</code>, <code>max_iter</code> | | ClassifierModule | <code>latent</code> | <code>algorithms/latent=classifier</code> | <code>model</code>, <code>max_iter</code>, <code>init_seed</code> | | DiffusionMapModule | <code>latent</code> | <code>algorithms/latent=diffusionmap</code> | <code>t</code>, <code>knn</code>, <code>decay</code>, <code>n_pca</code> | | LeidenModule | <code>latent</code> | <code>algorithms/latent=leiden</code> | <code>resolution</code>, <code>n_neighbors</code>, <code>backend</code>, <code>device</code> | | MDSModule | <code>latent</code> | <code>algorithms/latent=mds</code> | <code>ndim</code>, <code>seed</code>, <code>how</code>, <code>solver</code> | | MultiscalePHATEModule | <code>latent</code> | <code>algorithms/latent=multiscale_phate</code> | <code>scale</code>, <code>granularity</code>, <code>landmarks</code>, <code>knn</code> | | NoOpModule | <code>latent</code> | <code>algorithms/latent=noop</code> | -- | | PCAModule | <code>latent</code> | <code>algorithms/latent=pca</code> | -- | | PHATEModule | <code>latent</code> | <code>algorithms/latent=phate</code> | <code>knn</code>, <code>t</code>, <code>gamma</code>, <code>decay</code> | | PHATEModule | <code>latent</code> | <code>algorithms/latent=phate_torchdr</code> | <code>knn</code>, <code>t</code>, <code>gamma</code>, <code>decay</code> | | DiffusionMapModule | <code>latent</code> | <code>algorithms/latent=spectral_clustering</code> | <code>t</code>, <code>knn</code>, <code>decay</code>, <code>n_pca</code> | | TSNEModule | <code>latent</code> | <code>algorithms/latent=tsne</code> | <code>perplexity</code>, <code>n_iter_early</code>, <code>n_iter_late</code>, <code>learning_rate</code> | | TSNEModule | <code>latent</code> | <code>algorithms/latent=tsne_torchdr</code> | <code>perplexity</code>, <code>fit_fraction</code>, <code>backend</code>, <code>device</code> | | UMAPModule | <code>latent</code> | <code>algorithms/latent=umap</code> | <code>n_neighbors</code>, <code>min_dist</code>, <code>n_epochs</code>, <code>metric</code> | | UMAPModule | <code>latent</code> | <code>algorithms/latent=umap_torchdr</code> | <code>n_neighbors</code>, <code>min_dist</code>, <code>fit_fraction</code>, <code>backend</code> |</p> <pre><code>### FoundationEncoder Pattern\n\nFrozen pretrained models also use LatentModule \u2014 `fit()` is a no-op and `transform()` wraps the model's forward pass. This is a usage convention, not a separate class. Implementations live in `manylatents-omics/manylatents/dogma/encoders/` (Evo2, ESM3, Orthrus, AlphaGenome).\n\n### Optional Methods\n\nIf your algorithm uses a kernel-based approach, implement these to enable module-level metrics:\n\n```python\ndef kernel_matrix(self, ignore_diagonal=False) -&gt; np.ndarray:\n    \"\"\"Raw similarity matrix (N x N).\"\"\"\n    ...\n\ndef affinity_matrix(self, ignore_diagonal=False, use_symmetric=False) -&gt; np.ndarray:\n    \"\"\"Normalized transition matrix.\"\"\"\n    ...\n```\n\nThis enables metrics like `KernelMatrixSparsity`, `AffinitySpectrum`, and `ConnectedComponents`.\n\n### Running\n\n```bash\nuv run python -m manylatents.main algorithms/latent=pca data=swissroll\n```\n\n### Adding a New LatentModule\n\n1. Create `manylatents/algorithms/latent/your_algo.py` inheriting from `LatentModule`\n2. Implement `fit(x, y=None)` and `transform(x)`\n3. Create `manylatents/configs/algorithms/latent/your_algo.yaml` with `_target_`\n4. Import in `manylatents/algorithms/latent/__init__.py`\n</code></pre> LightningModule <p>|---|---|---|---| | Reconstruction | <code>lightning</code> | <code>algorithms/lightning=aanet_reconstruction</code> | <code>datamodule</code>, <code>network</code>, <code>optimizer</code>, <code>loss</code> | | Reconstruction | <code>lightning</code> | <code>algorithms/lightning=ae_reconstruction</code> | <code>datamodule</code>, <code>network</code>, <code>optimizer</code>, <code>loss</code> | | HFTrainerModule | <code>lightning</code> | <code>algorithms/lightning=hf_trainer</code> | <code>config</code> | | LatentODE | <code>lightning</code> | <code>algorithms/lightning=latent_ode</code> | <code>datamodule</code>, <code>network</code>, <code>optimizer</code>, <code>loss</code> |</p> <pre><code>### Pattern\n\nAll LightningModule algorithms follow the same pattern:\n\n```python\nclass MyTrainableAlgorithm(LightningModule):\n    def __init__(self, network, optimizer, loss, datamodule=None, init_seed=42):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"datamodule\", \"network\", \"loss\"])\n        self.network_config = network\n        self.optimizer_config = optimizer\n        self.loss_fn = loss\n\n    def setup(self, stage=None):\n        # Deferred network construction \u2014 input_dim from datamodule\n        input_dim = self.trainer.datamodule.data_dim\n        self.network = hydra.utils.instantiate(self.network_config, input_dim=input_dim)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.network(batch)\n        return self.loss_fn(outputs, batch)\n\n    def encode(self, x):\n        return self.network.encode(x)\n\n    def configure_optimizers(self):\n        return hydra.utils.instantiate(self.optimizer_config, params=self.parameters())\n```\n\nKey conventions:\n\n- `save_hyperparameters(ignore=[\"datamodule\", \"network\", \"loss\"])` \u2014 Lightning can't serialize nn.Modules\n- `setup()` defers network construction until `input_dim` is known from the datamodule\n- `encode()` extracts embeddings for evaluation after training\n- Use the project's `MSELoss` from `manylatents.algorithms.lightning.losses.mse`, not `torch.nn.MSELoss`\n\n### Latent ODE\n\nThe `LatentODE` algorithm integrates neural ODEs for learning continuous-time dynamics in latent space:\n\n```bash\nuv run python -m manylatents.main \\\n  algorithms/lightning=latent_ode \\\n  data=swissroll \\\n  trainer.max_epochs=10\n```\n\nConfiguration supports custom integration times and ODE solver options via `torchdiffeq`.\n\n### Running\n\n```bash\nuv run python -m manylatents.main \\\n  algorithms/lightning=ae_reconstruction \\\n  data=swissroll \\\n  trainer.max_epochs=10\n\n# Fast dev run for testing\nuv run python -m manylatents.main \\\n  algorithms/lightning=ae_reconstruction \\\n  data=swissroll \\\n  trainer.fast_dev_run=true\n```\n\n### Adding a New LightningModule\n\n1. Create `manylatents/algorithms/lightning/your_algo.py` inheriting from `LightningModule`\n2. Implement `setup()`, `training_step()`, `encode()`, `configure_optimizers()`\n3. Use `self.save_hyperparameters(ignore=[\"datamodule\", \"network\", \"loss\"])`\n4. Create config in `manylatents/configs/algorithms/lightning/your_algo.yaml`\n5. Test with `trainer.fast_dev_run=true`\n</code></pre> Networks &amp; Losses"},{"location":"algorithms/#fittransform-algorithms","title":"fit/transform Algorithms","text":"<p><code>LatentModule</code> is the base class for non-neural algorithms. Subclass it, implement <code>fit()</code> and <code>transform()</code>, and you're done.</p> <pre><code>from manylatents.algorithms.latent.latent_module_base import LatentModule\n\nclass MyAlgorithm(LatentModule):\n    def __init__(self, n_components=2, my_param=1.0, **kwargs):\n        super().__init__(n_components=n_components, **kwargs)\n        self.my_param = my_param\n\n    def fit(self, x, y=None):\n        self._is_fitted = True\n\n    def transform(self, x):\n        return x[:, :self.n_components]\n</code></pre>"},{"location":"algorithms/#available-algorithms","title":"Available Algorithms","text":"<p>| algorithm | type | config | key params |</p>"},{"location":"algorithms/#trainable-algorithms","title":"Trainable Algorithms","text":"<p>Neural network algorithms use PyTorch Lightning's <code>LightningModule</code> with training loops. Implement <code>setup()</code>, <code>training_step()</code>, <code>encode()</code>, and <code>configure_optimizers()</code>.</p>"},{"location":"algorithms/#available-algorithms_1","title":"Available Algorithms","text":"<p>| algorithm | type | config | key params |</p>"},{"location":"algorithms/#network-architectures","title":"Network Architectures","text":"<p>Networks are <code>nn.Module</code> classes used by LightningModule algorithms. They define the architecture; the LightningModule wraps the training logic.</p>"},{"location":"algorithms/#available-networks","title":"Available Networks","text":"Network Class Config Description Autoencoder <code>Autoencoder</code> <code>algorithms/lightning/network=autoencoder</code> Symmetric encoder-decoder with configurable layers AANet <code>AANet</code> <code>algorithms/lightning/network=aanet</code> Archetypal analysis network LatentODENetwork <code>LatentODENetwork</code> (configured via <code>latent_ode.yaml</code>) ODE function for continuous dynamics"},{"location":"algorithms/#autoencoder-config","title":"Autoencoder Config","text":"<pre><code># configs/algorithms/lightning/network/autoencoder.yaml\n_target_: manylatents.algorithms.lightning.networks.autoencoder.Autoencoder\ninput_dim: ???  # Set by setup() from datamodule\nhidden_dims: [128, 64]\nlatent_dim: 2\nactivation: relu\n</code></pre>"},{"location":"algorithms/#loss-functions","title":"Loss Functions","text":"<p>Use the project's loss functions, not PyTorch's directly.</p> Loss Class Config Description MSELoss <code>MSELoss</code> <code>algorithms/lightning/loss=default</code> Reconstruction loss GeometricLoss <code>GeometricLoss</code> <code>algorithms/lightning/loss=ae_dim</code> Dimensionality-aware loss GeometricLoss <code>GeometricLoss</code> <code>algorithms/lightning/loss=ae_neighbors</code> Neighborhood-preserving loss GeometricLoss <code>GeometricLoss</code> <code>algorithms/lightning/loss=ae_shape</code> Shape-preserving loss <p>The project's <code>MSELoss</code> (from <code>manylatents.algorithms.lightning.losses.mse</code>) accepts <code>(outputs, targets, **kwargs)</code>, unlike <code>torch.nn.MSELoss</code>. Always use the project's version.</p>"},{"location":"algorithms/#optimizer-config","title":"Optimizer Config","text":"<pre><code># configs/algorithms/lightning/optimizer/adam.yaml\n_target_: torch.optim.Adam\n_partial_: true\nlr: 0.001\n</code></pre> <p>The <code>_partial_: true</code> flag creates a partial that receives <code>params=</code> from <code>configure_optimizers()</code>.</p>"},{"location":"algorithms/#composing-a-full-config","title":"Composing a Full Config","text":"<pre><code># configs/algorithms/lightning/ae_reconstruction.yaml\n_target_: manylatents.algorithms.lightning.reconstruction.Reconstruction\n_recursive_: false\ndatamodule: ${data}\nnetwork:\n  _target_: manylatents.algorithms.lightning.networks.autoencoder.Autoencoder\n  input_dim: null  # inferred from data in setup()\n  hidden_dims: [512, 256, 128]\n  latent_dim: 50\n  activation: relu\n  batchnorm: true\n  dropout: 0.1\noptimizer:\n  _target_: torch.optim.Adam\n  _partial_: true\n  lr: 0.001\nloss:\n  _target_: manylatents.algorithms.lightning.losses.mse.MSELoss\n</code></pre> <p><code>_recursive_: false</code> prevents Hydra from eagerly instantiating nested configs \u2014 the <code>Reconstruction</code> module handles deferred instantiation in <code>setup()</code> once <code>input_dim</code> is known from the datamodule.</p>"},{"location":"api_usage/","title":"API Usage Guide","text":"<p>The ManyLatents programmatic API enables workflow integration and in-memory data chaining.</p>"},{"location":"api_usage/#quick-start","title":"Quick Start","text":"<pre><code>from manylatents.api import run\n\n# Single algorithm run\nresult = run(\n    data='swissroll',\n    algorithms={'latent': {'_target_': 'manylatents.algorithms.latent.pca.PCAModule', 'n_components': 10}}\n)\n\nembeddings = result['embeddings']  # numpy array\nscores = result['scores']          # dict of metrics\n</code></pre>"},{"location":"api_usage/#chained-workflows","title":"Chained Workflows","text":"<p>Chain multiple algorithms by passing the output of one as input to another:</p> <pre><code>from manylatents.api import run\n\n# Step 1: Initial dimensionality reduction\nresult1 = run(\n    data='swissroll',\n    algorithms={'latent': {'_target_': 'manylatents.algorithms.latent.pca.PCAModule', 'n_components': 50}}\n)\n\n# Step 2: Chain to another algorithm\nresult2 = run(\n    input_data=result1['embeddings'],\n    algorithms={'latent': {'_target_': 'manylatents.algorithms.latent.umap.UMAPModule', 'n_components': 2}}\n)\n\nfinal_embeddings = result2['embeddings']\n</code></pre> <p>Note: Embeddings are automatically converted to numpy arrays by the evaluation system.</p>"},{"location":"api_usage/#available-algorithms","title":"Available Algorithms","text":""},{"location":"api_usage/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<ul> <li>PCA: <code>manylatents.algorithms.latent.pca.PCAModule</code></li> <li>t-SNE: <code>manylatents.algorithms.latent.tsne.TSNEModule</code></li> <li>UMAP: <code>manylatents.algorithms.latent.umap.UMAPModule</code></li> <li>PHATE: <code>manylatents.algorithms.latent.phate.PHATEModule</code></li> </ul>"},{"location":"api_usage/#api-reference","title":"API Reference","text":""},{"location":"api_usage/#runinput_datanone-overrides","title":"<code>run(input_data=None, **overrides)</code>","text":"<p>Execute a dimensionality reduction algorithm.</p> <p>Parameters:</p> <ul> <li><code>input_data</code> (np.ndarray, optional): In-memory data array of shape <code>(n_samples, n_features)</code>. If provided, this data is used instead of loading from a dataset.</li> <li><code>**overrides</code>: Configuration overrides (e.g., <code>data='swissroll'</code>, <code>algorithms={...}</code>)</li> </ul> <p>Returns:</p> <p>Dictionary with keys: - <code>embeddings</code>: Computed embeddings (numpy array) - <code>label</code>: Labels from dataset (if available) - <code>metadata</code>: Run metadata dictionary - <code>scores</code>: Evaluation metrics (if enabled)</p> <p>Examples:</p> <pre><code># Using a built-in dataset\nresult = run(data='swissroll', algorithms={'latent': 'pca'})\n\n# Using in-memory data\nimport numpy as np\nmy_data = np.random.randn(1000, 100).astype(np.float32)\nresult = run(input_data=my_data, algorithms={'latent': 'pca'})\n\n# Chaining algorithms\nresult2 = run(input_data=result['embeddings'], algorithms={'latent': 'umap'})\n</code></pre>"},{"location":"api_usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api_usage/#custom-configuration","title":"Custom Configuration","text":"<pre><code>result = run(\n    data='swissroll',\n    algorithms={\n        'latent': {\n            '_target_': 'manylatents.algorithms.latent.umap.UMAPModule',\n            'n_components': 2,\n            'n_neighbors': 15,\n            'min_dist': 0.1\n        }\n    }\n)\n</code></pre>"},{"location":"api_usage/#disabling-features-for-speed","title":"Disabling Features for Speed","text":"<pre><code># Disable W&amp;B logging\nresult = run(data='swissroll', algorithms={'latent': 'pca'}, debug=True)\n\n# Skip evaluation metrics\nresult = run(data='swissroll', algorithms={'latent': 'pca'}, metrics=None)\n</code></pre>"},{"location":"api_usage/#data-format-requirements","title":"Data Format Requirements","text":"<ul> <li>Input: numpy.ndarray with dtype <code>float32</code> or <code>float64</code></li> <li>Shape: <code>(n_samples, n_features)</code></li> <li>Output: numpy array (tensor conversion is handled automatically)</li> </ul>"},{"location":"api_usage/#multi-step-example","title":"Multi-Step Example","text":"<pre><code>from manylatents.api import run\n\n# Progressive dimensionality reduction\nsteps = [\n    ('PCA 100D', 'manylatents.algorithms.latent.pca.PCAModule', 100),\n    ('PCA 50D', 'manylatents.algorithms.latent.pca.PCAModule', 50),\n    ('UMAP 2D', 'manylatents.algorithms.latent.umap.UMAPModule', 2),\n]\n\n# Initial data\ncurrent_data = run(\n    data='swissroll',\n    algorithms={'latent': {'_target_': steps[0][1], 'n_components': steps[0][2]}}\n)\n\n# Chain subsequent steps\nfor name, target, n_comp in steps[1:]:\n    print(f\"Running {name}...\")\n    current_data = run(\n        input_data=current_data['embeddings'],\n        algorithms={'latent': {'_target_': target, 'n_components': n_comp}}\n    )\n\nprint(f\"Final shape: {current_data['embeddings'].shape}\")\n</code></pre>"},{"location":"api_usage/#implementation-details","title":"Implementation Details","text":"<p>The in-memory data pipeline uses: - <code>PrecomputedDataModule</code>: Accepts <code>data</code> parameter for numpy arrays - <code>InMemoryDataset</code>: Wraps arrays in LatentOutputs format - Compatible with all ManyLatents metrics, callbacks, and visualizations</p>"},{"location":"api_usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api_usage/#common-issues","title":"Common Issues","text":"<p>\"PrecomputedDataModule requires either a 'path' or 'data' argument\"</p> <p>Provide either <code>data='dataset_name'</code> OR <code>input_data=array</code>, not both or neither.</p>"},{"location":"api_usage/#see-also","title":"See Also","text":"<ul> <li>Testing Guide</li> <li>Metrics</li> </ul>"},{"location":"cache/","title":"Cache Protocol","text":"<p>All metrics share a single <code>cache</code> dict. The config sleuther discovers k-values from metric configs and pre-warms kNN and eigenvalues before any metric runs.</p> <pre><code># this happens automatically inside evaluate_embeddings()\ncache = {}\ncompute_knn(high_dim_data, k=25, cache=cache)    # computed once\ncompute_knn(embeddings,     k=25, cache=cache)    # computed once\ncompute_eigenvalues(module, cache=cache)           # computed once\n\n# every metric reuses the same cache\ntrustworthiness(emb, dataset=ds, cache=cache)\ncontinuity(emb, dataset=ds, cache=cache)\n</code></pre> <p><code>compute_knn</code> selects the fastest available backend: FAISS-GPU &gt; FAISS-CPU &gt; sklearn.</p>"},{"location":"cache/#how-it-works","title":"How It Works","text":"<p><code>evaluate_embeddings()</code> uses <code>extract_k_requirements()</code> to discover all <code>k</code>/<code>n_neighbors</code> values from metric configs, then calls <code>prewarm_cache()</code> to compute kNN once with <code>max(k)</code>:</p> <ol> <li>Sleuther extracts requirements from metric configs</li> <li>Pre-warm computes kNN and eigenvalues at optimal k values</li> <li>Metrics receive the shared cache \u2014 <code>compute_knn()</code> slices cached results for smaller k values</li> </ol>"},{"location":"cache/#metric-expansion","title":"Metric Expansion","text":"<p>List-valued parameters expand via Cartesian product through <code>flatten_and_unroll_metrics()</code>:</p> <pre><code>trustworthiness:\n  _target_: manylatents.metrics.trustworthiness.Trustworthiness\n  _partial_: true\n  n_neighbors: [5, 10, 20]\n\n# expands to three evaluations:\n# embedding.trustworthiness__n_neighbors_5\n# embedding.trustworthiness__n_neighbors_10\n# embedding.trustworthiness__n_neighbors_20\n</code></pre> <p>All expanded k-values contribute to the shared cache \u2014 one kNN computation covers the entire sweep.</p>"},{"location":"cache/#extension-metrics","title":"Extension Metrics","text":"<p>Extension metrics that don't accept <code>cache=</code> are handled gracefully via a <code>TypeError</code> fallback. A warning is logged suggesting the extension add <code>cache=None</code> to its signature.</p>"},{"location":"callbacks/","title":"Callbacks","text":"<p>manyLatents has two callback systems: embedding callbacks for post-embedding processing, and trainer callbacks (Lightning) for training-time hooks.</p> ArchitectureEmbedding CallbacksTrainer Callbacks"},{"location":"callbacks/#callback-hierarchy","title":"Callback Hierarchy","text":"<pre><code>BaseCallback (ABC)\n\u251c\u2500\u2500 on_experiment_start(cfg)\n\u251c\u2500\u2500 on_experiment_end()\n\u251c\u2500\u2500 on_latent_end(dataset, embeddings)\n\u251c\u2500\u2500 on_training_start()\n\u2514\u2500\u2500 on_training_end()\n\nEmbeddingCallback(BaseCallback, ABC)\n\u251c\u2500\u2500 on_latent_end(dataset, embeddings)  \u2190 abstract, must implement\n\u251c\u2500\u2500 register_output(key, output)        \u2190 store results for downstream\n\u2514\u2500\u2500 callback_outputs: dict              \u2190 accumulated outputs\n\nlightning.Callback                      \u2190 PyTorch Lightning's callback\n\u251c\u2500\u2500 on_fit_start()\n\u251c\u2500\u2500 on_train_batch_end()\n\u251c\u2500\u2500 on_train_epoch_end()\n\u251c\u2500\u2500 on_validation_end()\n\u2514\u2500\u2500 ...\n</code></pre> <p><code>EmbeddingCallback</code> runs after embeddings are computed (for both LatentModule and LightningModule). Lightning <code>Callback</code> runs during training (LightningModule only).</p>"},{"location":"callbacks/#instantiation","title":"Instantiation","text":"<p>Callbacks are instantiated from two config groups and routed by type:</p> <pre><code>def instantiate_callbacks(trainer_cb_cfg, embedding_cb_cfg):\n    lightning_cbs, embedding_cbs = [], []\n\n    for name, cfg in trainer_cb_cfg.items():\n        cb = hydra.utils.instantiate(cfg)\n        if isinstance(cb, Callback):\n            lightning_cbs.append(cb)\n\n    for name, cfg in embedding_cb_cfg.items():\n        cb = hydra.utils.instantiate(cfg)\n        if isinstance(cb, EmbeddingCallback):\n            embedding_cbs.append(cb)\n\n    return lightning_cbs, embedding_cbs\n</code></pre>"},{"location":"callbacks/#config-structure","title":"Config Structure","text":"<pre><code># configs/callbacks/default.yaml\ndefaults:\n  - trainer: null         # Lightning callbacks (probing, etc.)\n  - embedding: null       # Embedding callbacks (save, plot, wandb)\n  - _self_\n</code></pre> <pre><code># configs/callbacks/embedding/default.yaml\ndefaults:\n  - save_embeddings\n  - plot_embeddings\n  - wandb_log_scores\n  - _self_\n</code></pre>"},{"location":"callbacks/#execution-flow","title":"Execution Flow","text":"<p>In <code>run_algorithm()</code>:</p> <ol> <li>Callbacks instantiated from config</li> <li>Lightning callbacks passed to <code>Trainer(callbacks=[...])</code></li> <li>Algorithm executes (fit/transform or trainer.fit)</li> <li>Embeddings wrapped as <code>LatentOutputs</code> dict</li> <li>Metrics evaluated</li> <li>Each embedding callback's <code>on_latent_end()</code> called with dataset + embeddings</li> <li>Callback outputs merged into the embeddings dict</li> </ol> <pre><code>for cb in embedding_cbs:\n    cb_result = cb.on_latent_end(dataset=datamodule.test_dataset, embeddings=embeddings)\n    if isinstance(cb_result, dict):\n        callback_outputs.update(cb_result)\n</code></pre>"},{"location":"callbacks/#latentoutputs","title":"LatentOutputs","text":"<p>The standard interchange format passed to all embedding callbacks:</p> <pre><code>LatentOutputs = dict[str, Any]\n# Required: \"embeddings\" (np.ndarray) \u2014 shape (n, d)\n# Optional: \"label\", \"metadata\", \"scores\", \"callback_outputs\"\n# Custom keys: \"trajectories\", cluster assignments, velocity fields, etc.\n</code></pre> <p><code>validate_latent_outputs()</code> checks the required key exists.</p>"},{"location":"callbacks/#output-types","title":"Output Types","text":"<p>Algorithms populate different keys depending on what they produce:</p> Key Shape Description <code>\"embeddings\"</code> <code>(n, d)</code> Point cloud in latent space (default, chainable in pipelines) <code>\"trajectories\"</code> <code>(n_bins, n_traj, d)</code> Flow paths from trajectory inference methods <code>\"label\"</code> <code>(n,)</code> Ground truth labels <code>\"scores\"</code> <code>dict</code> Evaluation metrics <code>\"metadata\"</code> <code>dict</code> Algorithm info and runtime metadata <p>The <code>\"embeddings\"</code> key is the standard primary output used by metrics, plotting, and pipeline chaining. Trajectory methods populate both <code>\"embeddings\"</code> (e.g., endpoint positions) and <code>\"trajectories\"</code> for the full flow data.</p> <p><code>SaveEmbeddings</code> automatically persists any additional keys as separate <code>.npy</code> or <code>.json</code> files when <code>save_additional_outputs: true</code>.</p>"},{"location":"callbacks/#saveembeddings","title":"SaveEmbeddings","text":"<p>Saves embeddings to disk in CSV or NPY format. Optionally saves metric tables (scalar summary and per-sample).</p> <pre><code># configs/callbacks/embedding/save_embeddings.yaml\nsave_embeddings:\n  _target_: manylatents.callbacks.embedding.save_embeddings.SaveEmbeddings\n  save_dir: ${hydra:runtime.output_dir}\n  save_format: \"csv\"\n  experiment_name: ${name}\n  save_metric_tables: false\n</code></pre> Parameter Default Description <code>save_dir</code> Hydra output dir Base directory for saved files <code>save_format</code> <code>\"csv\"</code> Format: <code>\"csv\"</code> or <code>\"npy\"</code> <code>save_metric_tables</code> <code>false</code> Save separate scalar + per-sample metric CSVs <code>save_additional_outputs</code> <code>false</code> Save non-embedding keys as separate files <p>When running under Geomancer orchestration, also writes to the shared metrics directory via <code>atomic_writer</code>.</p>"},{"location":"callbacks/#plotembeddings","title":"PlotEmbeddings","text":"<p>Creates 2D scatter plots of embeddings with customizable colormaps and optional WandB upload.</p> <pre><code># configs/callbacks/embedding/plot_embeddings.yaml\nplot_embeddings:\n  _target_: manylatents.callbacks.embedding.plot_embeddings.PlotEmbeddings\n  save_dir: ${hydra:runtime.output_dir}\n  experiment_name: \"${name}.png\"\n  figsize: [8, 6]\n  label_col: Population\n  legend: false\n  color_by_score: null\n</code></pre>"},{"location":"callbacks/#colormap-resolution","title":"Colormap Resolution","text":"<p>PlotEmbeddings resolves colormaps from multiple sources (highest priority first):</p> <ol> <li>User overrides \u2014 <code>cmap_override</code>, <code>is_categorical_override</code> in config</li> <li>Metric-declared \u2014 <code>scores[\"&lt;metric&gt;__viz\"]</code> containing a <code>ColormapInfo</code></li> <li>Dataset-provided \u2014 via the <code>ColormapProvider</code> protocol</li> <li>Defaults \u2014 <code>\"viridis\"</code></li> </ol> <p>Datasets can implement <code>ColormapProvider</code> to declare their preferred visualization:</p> <pre><code>class MyDataset(ColormapProvider):\n    def get_colormap_info(self) -&gt; ColormapInfo:\n        return ColormapInfo(\n            cmap={\"A\": \"#ff0000\", \"B\": \"#00ff00\"},\n            label_names={0: \"Class A\", 1: \"Class B\"},\n            is_categorical=True,\n        )\n</code></pre>"},{"location":"callbacks/#coloring-by-score","title":"Coloring by Score","text":"<p>Color points by a metric value instead of labels:</p> <pre><code>plot_embeddings:\n  color_by_score: \"embedding.local_intrinsic_dimensionality\"\n  legend: false  # Uses colorbar instead\n</code></pre>"},{"location":"callbacks/#wandblogscores","title":"WandbLogScores","text":"<p>Logs metric scores to WandB in three formats:</p> <pre><code># configs/callbacks/embedding/wandb_log_scores.yaml\nwandb_log_scores:\n  _target_: manylatents.callbacks.embedding.wandb_log_scores.WandbLogScores\n</code></pre> Log Type WandB Key Content Summary scalars <code>{tag}/metric_name</code> 0-D metrics as <code>wandb.log()</code> Per-sample table <code>{tag}/per_sample_metrics</code> 1-D arrays as <code>wandb.Table</code> k-curve tables <code>{tag}/metric__k_curve_table</code> Swept <code>n_neighbors</code> values grouped into tables <p>k-curve tables automatically detect metrics swept over <code>n_neighbors</code> (e.g., <code>trustworthiness__n_neighbors_5</code>, <code>_10</code>, <code>_20</code>) and group them into a single curve.</p>"},{"location":"callbacks/#loadingsanalysiscallback","title":"LoadingsAnalysisCallback","text":"<p>Analyzes shared vs modality-specific components in multi-modal loadings (e.g., DNA + RNA + Protein fusion).</p> <pre><code>callbacks:\n  embedding:\n    loadings:\n      _target_: manylatents.callbacks.embedding.loadings_analysis.LoadingsAnalysisCallback\n      modality_dims: [1920, 256, 1536]\n      modality_names: [dna, rna, protein]\n      threshold: 0.1\n</code></pre> <p>Requires the algorithm module to have a <code>get_loadings()</code> method (e.g., <code>MergingModule</code> with <code>concat_pca</code>).</p>"},{"location":"callbacks/#lightning-callbacks","title":"Lightning Callbacks","text":"<p>Trainer callbacks extend <code>lightning.Callback</code> and run during the training loop. They are passed to <code>Trainer(callbacks=[...])</code>.</p>"},{"location":"callbacks/#activationtrajectorycallback","title":"ActivationTrajectoryCallback","text":"<p>The primary trainer callback. Extracts activations from model layers at configurable triggers and computes diffusion operators to track representation geometry.</p> <pre><code># configs/callbacks/trainer/probe.yaml\nprobe:\n  _target_: manylatents.lightning.callbacks.activation_tracker.ActivationTrajectoryCallback\n  layer_specs:\n    - _target_: manylatents.lightning.hooks.LayerSpec\n      path: \"transformer.h[-1]\"\n      extraction_point: \"output\"\n      reduce: \"mean\"\n  trigger:\n    _target_: manylatents.lightning.callbacks.activation_tracker.ProbeTrigger\n    every_n_steps: 500\n    on_checkpoint: true\n    on_validation_end: true\n  gauge:\n    _target_: manylatents.callbacks.diffusion_operator.DiffusionGauge\n    knn: 15\n    alpha: 1.0\n    symmetric: false\n  log_to_wandb: true\n</code></pre> <p>See Probing for full documentation of layer specs, triggers, gauge configuration, and programmatic access.</p>"},{"location":"callbacks/#adding-a-trainer-callback","title":"Adding a Trainer Callback","text":"<ol> <li>Create a class extending <code>lightning.Callback</code></li> <li>Implement the relevant hooks (<code>on_train_batch_end</code>, <code>on_train_epoch_end</code>, etc.)</li> <li>Create a config in <code>configs/callbacks/trainer/your_callback.yaml</code></li> <li>Add to your experiment config:</li> </ol> <pre><code>callbacks:\n  trainer:\n    your_callback:\n      _target_: manylatents.your_module.YourCallback\n      param: value\n</code></pre>"},{"location":"callbacks/#adding-an-embedding-callback","title":"Adding an Embedding Callback","text":"<ol> <li>Create a class extending <code>EmbeddingCallback</code></li> <li>Implement <code>on_latent_end(self, dataset, embeddings) -&gt; Any</code></li> <li>Use <code>self.register_output(key, value)</code> to store results</li> <li>Create a config in <code>configs/callbacks/embedding/your_callback.yaml</code></li> <li>Add to the embedding defaults or your experiment config</li> </ol>"},{"location":"contributing/","title":"Contributing to manyLatents","text":"<p>Guidelines for adding new components and ensuring they integrate correctly with the framework.</p>"},{"location":"contributing/#testing-philosophy","title":"Testing Philosophy","text":"<p>All contributions must pass automated tests on every pull request. CI validates:</p> <ol> <li>Unit tests \u2014 <code>pytest tests/</code> runs the full test suite</li> <li>CLI smoke tests \u2014 default LatentModule and LightningModule paths work</li> <li>Component discovery \u2014 if your PR touches algorithms, metrics, or data configs, CI auto-discovers all configs in that group and runs each one end-to-end</li> <li>Docs build \u2014 <code>mkdocs build --strict</code> passes</li> </ol>"},{"location":"contributing/#how-ci-detects-what-to-test","title":"How CI detects what to test","text":"<p>CI uses path-based filtering. If your PR modifies files under a component directory, the corresponding discovery test runs automatically:</p> Files changed CI runs <code>manylatents/algorithms/latent/**</code> or <code>configs/algorithms/latent/**</code> Discovers and smoke-tests every LatentModule config <code>manylatents/algorithms/lightning/**</code> or <code>configs/algorithms/lightning/**</code> Discovers and smoke-tests every LightningModule config <code>manylatents/metrics/**</code> or <code>configs/metrics/**</code> Discovers and smoke-tests every metric config <p>This means: if you add a new config YAML and its <code>_target_</code> doesn't resolve, or the algorithm crashes on synthetic data, CI will catch it.</p>"},{"location":"contributing/#adding-new-metrics","title":"Adding New Metrics","text":"<p>Every new metric follows 4 steps: wrapper \u2192 register \u2192 config \u2192 smoke test.</p>"},{"location":"contributing/#step-1-write-the-wrapper","title":"Step 1: Write the Wrapper","text":"<p>Follow the <code>Metric</code> protocol \u2014 <code>embeddings</code> first, then <code>dataset</code>, <code>module</code>, your params, and <code>cache</code>:</p> <pre><code># manylatents/metrics/your_metric.py\nfrom typing import Optional\nimport numpy as np\n\nfrom manylatents.metrics.registry import register_metric\nfrom manylatents.utils.metrics import compute_knn\n\n\n@register_metric(\n    aliases=[\"your_metric\"],\n    default_params={\"k\": 25},\n    description=\"Short description of what this metric measures\",\n)\ndef YourMetric(\n    embeddings: np.ndarray,\n    dataset=None,\n    module=None,\n    k: int = 25,\n    cache: Optional[dict] = None,\n) -&gt; float:\n    # Use compute_knn with cache for shared kNN computation\n    dists, indices = compute_knn(embeddings, k=k, cache=cache)\n    score = ...  # your computation\n    return float(score)\n</code></pre> <p>Return types: <code>float</code>, <code>tuple[float, np.ndarray]</code> (scalar + per-sample), or <code>dict[str, Any]</code> (structured).</p> <p>Evaluation context determines the config directory:</p> <ul> <li>Only needs original data? \u2192 <code>metrics/dataset/</code></li> <li>Compares original vs. reduced? \u2192 <code>metrics/embedding/</code></li> <li>Needs algorithm internals (graph, kernel)? \u2192 <code>metrics/module/</code></li> </ul>"},{"location":"contributing/#step-2-register-it","title":"Step 2: Register It","text":"<p>The <code>@register_metric</code> decorator (shown above) adds your metric to the dynamic registry with aliases, default params, and a description. This powers <code>list_metrics()</code>, auto-generated docs tables, and programmatic discovery.</p> <p>Import your metric in <code>manylatents/metrics/__init__.py</code> so the decorator fires at import time.</p>"},{"location":"contributing/#step-3-create-the-config","title":"Step 3: Create the Config","text":"<pre><code># manylatents/configs/metrics/embedding/your_metric.yaml\nyour_metric:\n  _target_: manylatents.metrics.your_metric.YourMetric\n  _partial_: True\n  k: 25\n</code></pre> <p>Configs are nested under the metric name with <code>_partial_: True</code> so Hydra binds the params at config time and the engine calls it with <code>embeddings</code>, <code>dataset</code>, <code>module</code>, and <code>cache</code> at runtime.</p>"},{"location":"contributing/#step-4-e2e-smoke-test","title":"Step 4: E2E Smoke Test","text":"<pre><code># Verify your metric runs end-to-end\nmanylatents algorithms/latent=pca data=swissroll \\\n  metrics/embedding=your_metric logger=none\n</code></pre> <p>CI will auto-discover your new config and test it if <code>manylatents/metrics/</code> or <code>configs/metrics/</code> files are changed.</p>"},{"location":"contributing/#adding-new-algorithms","title":"Adding New Algorithms","text":""},{"location":"contributing/#latentmodule-non-neural","title":"LatentModule (non-neural)","text":"<p>For algorithms without a training loop \u2014 fit/transform pattern:</p> <pre><code># manylatents/algorithms/latent/your_algorithm.py\nimport numpy as np\nfrom manylatents.algorithms.latent.latent_module_base import LatentModule\n\n\nclass YourAlgorithmModule(LatentModule):\n    \"\"\"Your dimensionality reduction algorithm.\"\"\"\n\n    def __init__(self, n_components: int = 2, **kwargs):\n        super().__init__()\n        self.n_components = n_components\n\n    def fit(self, X: np.ndarray) -&gt; None:\n        \"\"\"Fit the model on training data.\"\"\"\n        pass\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Transform data to low-dimensional space.\"\"\"\n        return self._compute_embeddings(X)\n</code></pre>"},{"location":"contributing/#lightningmodule-neural","title":"LightningModule (neural)","text":"<p>For algorithms that train with backprop:</p> <pre><code># manylatents/algorithms/lightning/your_network.py\nfrom lightning import LightningModule\n\n\nclass YourNetwork(LightningModule):\n    def __init__(self, ...):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"datamodule\", \"network\", \"loss\"])\n\n    def training_step(self, batch, batch_idx):\n        pass\n\n    def encode(self, x):\n        return embeddings\n\n    def configure_optimizers(self):\n        return ...\n</code></pre>"},{"location":"contributing/#create-config","title":"Create Config","text":"<pre><code># manylatents/configs/algorithms/latent/your_algorithm.yaml\n_target_: manylatents.algorithms.latent.your_algorithm.YourAlgorithmModule\nn_components: 2\n</code></pre>"},{"location":"contributing/#test-locally","title":"Test Locally","text":"<pre><code># LatentModule\nmanylatents algorithms/latent=your_algorithm data=swissroll \\\n  metrics=noop logger=none\n\n# LightningModule\nmanylatents algorithms/lightning=your_network data=swissroll \\\n  trainer.fast_dev_run=true metrics=noop logger=none\n</code></pre> <p>CI will auto-discover your new config and test it when you open a PR.</p>"},{"location":"contributing/#adding-new-datasets","title":"Adding New Datasets","text":"<pre><code># manylatents/data/your_dataset.py\nfrom typing import Optional\nfrom lightning import LightningDataModule\nfrom torch.utils.data import DataLoader\n\n\nclass YourDataModule(LightningDataModule):\n    def __init__(self, batch_size: int = 32, **kwargs):\n        super().__init__()\n        self.batch_size = batch_size\n\n    def setup(self, stage: Optional[str] = None):\n        self.train_dataset = ...\n        self.test_dataset = ...\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n</code></pre> <pre><code># manylatents/configs/data/your_dataset.yaml\n_target_: manylatents.data.your_dataset.YourDataModule\nbatch_size: 32\n</code></pre> <pre><code>manylatents data=your_dataset algorithms/latent=pca \\\n  metrics=noop logger=none\n</code></pre>"},{"location":"contributing/#ci-pipeline","title":"CI Pipeline","text":"<p>Three jobs run on every PR:</p> <p>test (Python 3.11 + 3.12 matrix): - <code>pytest tests/</code> \u2014 full unit test suite - CLI smoke test: LatentModule path (<code>experiment=single_algorithm</code>) - CLI smoke test: LightningModule path (<code>algorithms/lightning=ae_reconstruction</code>, <code>trainer.fast_dev_run=true</code>) - If <code>algorithms/latent/</code> changed \u2192 discovers and tests all LatentModule configs - If <code>algorithms/lightning/</code> changed \u2192 discovers and tests all LightningModule configs - If <code>metrics/</code> changed \u2192 discovers and tests all metric configs</p> <p>docs: - <code>scripts/check_docs_coverage.py</code> \u2014 verifies all <code>_target_</code> paths in configs are importable - <code>mkdocs build --strict</code> \u2014 verifies docs site builds cleanly</p> <p>publish (tags only): - Builds sdist + wheel, publishes to PyPI via trusted publishing</p>"},{"location":"contributing/#local-pre-submission-checklist","title":"Local Pre-submission Checklist","text":"<pre><code># Run unit tests\npytest tests/ -x -q\n\n# Smoke test your component\nmanylatents algorithms/latent=your_algo data=swissroll metrics=noop logger=none\n\n# Docs build (optional)\nmkdocs build --strict\n</code></pre>"},{"location":"contributing/#optional-dependencies","title":"Optional Dependencies","text":"<p>Some features require optional extras. If your contribution uses an optional dependency:</p> <ol> <li>Add it to the appropriate <code>[project.optional-dependencies]</code> group in <code>pyproject.toml</code></li> <li>Use lazy imports (<code>try/except ImportError</code>) so the core package doesn't break without it</li> <li>Add <code>pytest.importorskip()</code> or <code>@pytest.mark.skipif</code> to tests that need the dep</li> </ol> <p>Current extras: <code>hf</code> (transformers), <code>torchdr</code> (TorchDR + faiss), <code>jax</code> (JAX + diffrax + optax + ott-jax), <code>docs</code>.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Metrics reference</li> <li>API usage</li> <li>Cache protocol</li> <li>Issues: open a GitHub issue</li> </ul>"},{"location":"data/","title":"Data","text":"<p>manyLatents provides synthetic manifold datasets for benchmarking and a precomputed loader for custom data.</p> dataset config key params DLATree <code>data=dla_tree</code> n_dim=50, n_branch=10, branch_lengths=[300, 300, 25, 300, 300, 300, 25, 300, 300, 300] DLATreeFromGraph <code>data=dla_tree_from_graph</code> n_dim=100, sigma=0.5, mode=full DLATreeFromGraph <code>data=dla_tree_from_graph_nogaps</code> n_dim=100, sigma=0.5, mode=full GaussianBlob <code>data=gaussian_blobs</code> n_samples=1000, n_features=2, centers=5 MHI <code>data=mhi_split</code> cache_dir=${cache_dir}, mmap_mode=None, mode=split Precomputed <code>data=precomputed</code> path=None, label_col=None, mode=full SaddleSurface <code>data=saddle_surface</code> n_distributions=100, n_points_per_distribution=50, noise=0.1 SwissRoll <code>data=swissroll</code> n_distributions=10, n_points_per_distribution=100, noise=0.2 Torus <code>data=torus</code> n_points=1000, noise=0.1, major_radius=5.0 Text <code>data=wikitext</code> dataset_name=wikitext, dataset_config=wikitext-2-raw-v1, tokenizer_name=gpt2 <p>Domain-specific datasets (genomics, single-cell) are available via the manylatents-omics extension.</p>"},{"location":"data/#precomputed-data","title":"Precomputed Data","text":"<p>Load your own data from <code>.npy</code> or <code>.npz</code> files:</p> <pre><code>uv run python -m manylatents.main data=precomputed data.path=/path/to/data.npy algorithms/latent=umap\n</code></pre>"},{"location":"data/#sampling","title":"Sampling","text":"<p>Large datasets are subsampled before metric evaluation. Configure under <code>metrics.sampling</code>:</p> strategy config defaults FarthestPointSampling <code>sampling/farthest_point</code> seed=42, fraction=0.1 RandomSampling <code>sampling/random</code> seed=42, fraction=0.1 StratifiedSampling <code>sampling/stratified</code> stratify_by=population_label, seed=42, fraction=0.1"},{"location":"evaluation/","title":"Evaluation","text":"<p>How manyLatents dispatches, evaluates, and samples embeddings. The core engine lives in <code>experiment.py</code>.</p> DispatchSamplingCaching"},{"location":"evaluation/#algorithm-dispatch","title":"Algorithm Dispatch","text":"<p>manyLatents uses a two-level dispatch system to handle both LatentModule (fit/transform) and LightningModule (training loop) algorithms through a unified interface.</p>"},{"location":"evaluation/#algorithm-resolution","title":"Algorithm Resolution","text":"<p><code>run_algorithm()</code> determines which algorithm type to instantiate from the Hydra config:</p> <pre><code>if hasattr(cfg.algorithms, 'latent') and cfg.algorithms.latent is not None:\n    algorithm = instantiate_algorithm(cfg.algorithms.latent, datamodule)\nelif hasattr(cfg.algorithms, 'lightning') and cfg.algorithms.lightning is not None:\n    algorithm = instantiate_algorithm(cfg.algorithms.lightning, datamodule)\nelse:\n    raise ValueError(\"No algorithm specified in configuration\")\n</code></pre> <p>Only one of <code>algorithms/latent</code> or <code>algorithms/lightning</code> should be set per run. The config group determines which path is taken.</p>"},{"location":"evaluation/#execution-execute_step","title":"Execution: <code>execute_step()</code>","text":"<p><code>execute_step()</code> routes via <code>isinstance()</code> checks:</p> <pre><code>if isinstance(algorithm, LatentModule):\n    algorithm.fit(train_tensor, train_labels)\n    latents = algorithm.transform(test_tensor)\n\nelif isinstance(algorithm, LightningModule):\n    trainer.fit(algorithm, datamodule=datamodule)\n    latents = algorithm.encode(test_tensor)\n</code></pre> <p>LatentModule path: Direct <code>fit()</code> on training data, then <code>transform()</code> on test data. Labels are passed for supervised modules (e.g., <code>ClassifierModule</code>) and ignored by unsupervised ones.</p> <p>LightningModule path: Full Lightning training loop via <code>trainer.fit()</code>, optional pretrained checkpoint loading, model evaluation via <code>evaluate()</code>, then embedding extraction via <code>encode()</code>.</p>"},{"location":"evaluation/#evaluation-functoolssingledispatch","title":"Evaluation: <code>@functools.singledispatch</code>","text":"<p>The <code>evaluate()</code> function uses Python's <code>@functools.singledispatch</code> to dispatch on the first argument's type:</p> <pre><code>@functools.singledispatch\ndef evaluate(algorithm: Any, /, **kwargs):\n    raise NotImplementedError(...)\n\n@evaluate.register(dict)\ndef evaluate_embeddings(latent_outputs: dict, *, cfg, datamodule, **kwargs):\n    # Handles embedding-level metrics (trustworthiness, continuity, etc.)\n    ...\n\n@evaluate.register(LightningModule)\ndef evaluate_lightningmodule(algorithm: LightningModule, *, cfg, trainer, datamodule, **kwargs):\n    # Handles trainer.test() and model-specific metrics\n    ...\n</code></pre> Dispatch Type Handler Evaluates <code>dict</code> (LatentOutputs) <code>evaluate_embeddings()</code> Embedding metrics (trustworthiness, continuity, kNN preservation, etc.) <code>LightningModule</code> <code>evaluate_lightningmodule()</code> <code>trainer.test()</code> results + custom model metrics <p>Both paths are called during a LightningModule run: first <code>evaluate_lightningmodule</code> during <code>execute_step()</code>, then <code>evaluate_embeddings</code> on the extracted embeddings.</p>"},{"location":"evaluation/#pipeline-mode","title":"Pipeline Mode","text":"<p><code>run_pipeline()</code> chains multiple steps sequentially, where step N's output embeddings become step N+1's input. The dispatch logic is reused per step via <code>execute_step()</code>.</p> <pre><code># PCA (1000\u219250) \u2192 PHATE (50\u21922)\nuv run python -m manylatents.main experiment=my_pipeline\n</code></pre>"},{"location":"evaluation/#sampling-strategies","title":"Sampling Strategies","text":"<p>Large datasets can make metric computation expensive. manyLatents provides pluggable sampling strategies that subsample embeddings and datasets before evaluation.</p>"},{"location":"evaluation/#protocol","title":"Protocol","text":"<p>All strategies implement the <code>SamplingStrategy</code> protocol:</p> <pre><code>class SamplingStrategy(Protocol):\n    def sample(\n        self,\n        embeddings: np.ndarray,\n        dataset: object,\n        n_samples: Optional[int] = None,\n        fraction: Optional[float] = None,\n        seed: int = 42,\n    ) -&gt; Tuple[np.ndarray, object, np.ndarray]:\n        # Returns (subsampled_embeddings, subsampled_dataset, indices)\n        ...\n</code></pre> <p>The returned dataset is a deep copy with subsampled <code>data</code>, <code>latitude</code>, <code>longitude</code>, and <code>population_label</code> attributes (when present).</p>"},{"location":"evaluation/#available-strategies","title":"Available Strategies","text":"Strategy Config Use Case <code>RandomSampling</code> <code>sampling/random</code> Default. Uniform random without replacement <code>StratifiedSampling</code> <code>sampling/stratified</code> Preserves label distribution across strata <code>FarthestPointSampling</code> <code>sampling/farthest_point</code> Maximum coverage of embedding space <code>FixedIndexSampling</code> (programmatic) Reproducible cross-setting comparisons"},{"location":"evaluation/#configuration","title":"Configuration","text":"<p>Sampling is configured under <code>metrics.sampling</code> in Hydra:</p> <pre><code># Random (default)\nmetrics:\n  sampling:\n    _target_: manylatents.utils.sampling.RandomSampling\n    seed: 42\n    fraction: 0.1\n\n# Stratified by population label\nmetrics:\n  sampling:\n    _target_: manylatents.utils.sampling.StratifiedSampling\n    stratify_by: population_label\n    seed: 42\n    fraction: 0.1\n\n# Farthest point (O(n*k) \u2014 slower but better coverage)\nmetrics:\n  sampling:\n    _target_: manylatents.utils.sampling.FarthestPointSampling\n    seed: 42\n    fraction: 0.1\n</code></pre>"},{"location":"evaluation/#deterministic-indices","title":"Deterministic Indices","text":"<p><code>RandomSampling.get_indices()</code> precomputes indices without requiring data, enabling reproducible comparisons:</p> <pre><code>sampler = RandomSampling(seed=42)\nindices = sampler.get_indices(n_total=1000, fraction=0.1)\nnp.save('shared_indices.npy', indices)\n\n# Reuse across runs\nfixed = FixedIndexSampling(indices=np.load('shared_indices.npy'))\nemb_sub, ds_sub, _ = fixed.sample(embeddings, dataset)\n</code></pre>"},{"location":"evaluation/#how-sampling-integrates","title":"How Sampling Integrates","text":"<p>In <code>evaluate_embeddings()</code>, sampling runs before any metrics:</p> <pre><code>sampling_cfg = cfg.metrics.get(\"sampling\", None)\nif sampling_cfg is not None:\n    sampler = hydra.utils.instantiate(sampling_cfg)\n    emb_sub, ds_sub, _ = sampler.sample(embeddings, ds)\n</code></pre> <p>All metrics then operate on the subsampled data. If no sampling config is provided, metrics run on the full dataset.</p>"},{"location":"evaluation/#shared-cache","title":"Shared Cache","text":"<p>Several metrics need k-nearest neighbors, SVD decompositions, or eigenvalue computations. Computing these per-metric would be redundant. manyLatents pre-warms a shared <code>cache</code> dict and passes it to all metrics.</p>"},{"location":"evaluation/#how-it-works","title":"How It Works","text":"<p><code>evaluate_embeddings()</code> uses the config sleuther (<code>extract_k_requirements</code>) to discover all <code>k</code>/<code>n_neighbors</code> values from metric configs, then calls <code>prewarm_cache()</code> to compute kNN and eigenvalues once with <code>max(k)</code>:</p> <pre><code># 1. Sleuther extracts requirements from metric configs\nreqs = extract_k_requirements(metric_cfgs)\n# reqs = {\"emb_k\": {5, 10, 25}, \"data_k\": {10, 25}, \"spectral\": True}\n\n# 2. Pre-warm cache with optimal k values\ncache = prewarm_cache(metric_cfgs, embeddings, dataset, module)\n# cache is keyed by id(data) for kNN, \"eigenvalues\" for spectral\n\n# 3. All metrics receive the same cache dict\nresult = metric_fn(embeddings=emb, dataset=ds, module=module, cache=cache)\n</code></pre>"},{"location":"evaluation/#compute_knn-with-cache","title":"compute_knn with cache","text":"<p><code>compute_knn()</code> uses the <code>cache</code> dict to avoid redundant computation. If a cached result exists with <code>k &gt;= requested k</code>, it slices and returns immediately:</p> <pre><code>from manylatents.utils.metrics import compute_knn\n\ncache = {}\n# First call: computes kNN with k=25\ndists, idxs = compute_knn(data, k=25, cache=cache)\n\n# Second call: reuses cached result, slices to k=10\ndists, idxs = compute_knn(data, k=10, cache=cache)  # instant\n</code></pre> <p><code>compute_knn()</code> automatically selects the fastest backend: FAISS-GPU &gt; FAISS-CPU &gt; sklearn.</p>"},{"location":"evaluation/#svd-cache","title":"SVD Cache","text":"<p><code>compute_svd_cache()</code> batches local SVD computation with GPU acceleration (torch) when CUDA is available, falling back to CPU numpy. Results are stored in the same <code>cache</code> dict.</p>"},{"location":"evaluation/#metric-protocol","title":"Metric Protocol","text":"<p>All metrics receive <code>cache=</code> as a keyword argument. Metrics that need kNN call <code>compute_knn(..., cache=cache)</code> internally \u2014 the cache ensures no redundant computation. Extension metrics that don't accept <code>cache=</code> are handled gracefully via a <code>TypeError</code> fallback.</p>"},{"location":"evaluation/#metric-expansion","title":"Metric Expansion","text":"<p><code>flatten_and_unroll_metrics()</code> handles list-valued parameters via Cartesian product:</p> <pre><code># This config:\ntrustworthiness:\n  _target_: manylatents.metrics.trustworthiness.Trustworthiness\n  _partial_: true\n  n_neighbors: [5, 10, 20]\n\n# Expands to three separate evaluations:\n# embedding.trustworthiness__n_neighbors_5\n# embedding.trustworthiness__n_neighbors_10\n# embedding.trustworthiness__n_neighbors_20\n</code></pre> <p>This expansion happens before kNN extraction, so all k values from expanded metrics contribute to the shared cache.</p>"},{"location":"extensions/","title":"Extensions","text":"<p>manyLatents uses a modular extension system that allows domain-specific functionality to be installed as separate packages. Extensions integrate seamlessly through Python's namespace package system and Hydra's config composition.</p>"},{"location":"extensions/#available-extensions","title":"Available Extensions","text":""},{"location":"extensions/#manylatents-omics","title":"manylatents-omics","text":"<p>The genomics, population genetics, and single-cell extension for manyLatents.</p> <p>Repository: github.com/latent-reasoning-works/manylatents-omics</p> <p>Adds three submodules:</p> <ul> <li><code>manylatents.dogma</code> \u2014 Foundation model encoders (Evo2, ESM3, Orthrus, AlphaGenome) and fusion algorithms</li> <li><code>manylatents.popgen</code> \u2014 Population genetics data modules and metrics (GeographicPreservation, AdmixturePreservation)</li> <li><code>manylatents.singlecell</code> \u2014 Single-cell AnnData data modules</li> </ul> UsageArchitectureDevelopment"},{"location":"extensions/#installing","title":"Installing","text":""},{"location":"extensions/#quick-install-recommended","title":"Quick Install (Recommended)","text":"<pre><code>uv add git+https://github.com/latent-reasoning-works/manylatents-omics.git\n</code></pre>"},{"location":"extensions/#using-git-submodules","title":"Using Git Submodules","text":"<p>For contributors working on both core and extensions:</p> <pre><code>git submodule add https://github.com/latent-reasoning-works/manylatents-omics.git extensions/manylatents-omics\nuv add git+file://extensions/manylatents-omics\n</code></pre>"},{"location":"extensions/#development-workflow","title":"Development Workflow","text":"<p>Working FROM the manylatents-omics repo (recommended for omics development):</p> <pre><code>cd manylatents-omics\nuv sync  # Pulls manylatents from git automatically\nuv run python -m manylatents.main experiment=single_algorithm\n</code></pre> <p>Working FROM the manylatents repo (core development only):</p> <pre><code>cd manylatents\nuv sync\nuv run python -m manylatents.main experiment=single_algorithm\n</code></pre> <p>Extensions are auto-discovered via entry-points. Use <code>manylatents.main</code> regardless of which repo you're working from.</p>"},{"location":"extensions/#using-extensions-in-code","title":"Using Extensions in Code","text":"<p>Once installed, extension features are available through the <code>manylatents</code> namespace:</p> <pre><code># Core imports (always available)\nfrom manylatents.data import SwissRoll\nfrom manylatents.algorithms.latent import PCAModule\n\n# Extension imports (available when manylatents-omics is installed)\nfrom manylatents.popgen.data import HGDPDataset\nfrom manylatents.popgen.metrics import GeographicPreservation\nfrom manylatents.dogma.encoders import Evo2Encoder\nfrom manylatents.singlecell.data import AnnDataModule\n</code></pre>"},{"location":"extensions/#using-extensions-with-hydra","title":"Using Extensions with Hydra","text":"<p>Extensions integrate directly with Hydra configs:</p> <pre><code>uv run python -m manylatents.main \\\n  data=hgdp_1kgp \\\n  algorithms/latent=pca \\\n  metrics=genetic_metrics \\\n  logger=wandb\n</code></pre>"},{"location":"extensions/#checking-whats-installed","title":"Checking What's Installed","text":"<pre><code>import pkgutil\nimport manylatents\n\nfor importer, modname, ispkg in pkgutil.iter_modules(manylatents.__path__):\n    print(f\"- {modname}\")\n</code></pre>"},{"location":"extensions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"extensions/#hydra-config-discovery-error","title":"Hydra Config Discovery Error","text":"<p>Problem: <code>ConfigAttributeError: Key 'experiment' is not in struct</code></p> <p>Cause: Hydra SearchPathPlugin not being discovered.</p> <p>Solutions:</p> <ol> <li>Ensure the extension is installed (not just cloned):     <pre><code>uv add git+https://github.com/latent-reasoning-works/manylatents-omics.git\n</code></pre></li> <li>If developing both packages, work from the omics repo</li> <li>Verify plugin discovery:     <pre><code>from hydra.core.plugins import Plugins\nfrom hydra.plugins.search_path_plugin import SearchPathPlugin\nplugins = list(Plugins.instance().discover(SearchPathPlugin))\nprint([p.__name__ for p in plugins])\n</code></pre></li> </ol>"},{"location":"extensions/#extension-not-found","title":"Extension Not Found","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'manylatents.omics'</code></p> <p>Solution: Install the extension: <pre><code>uv add git+https://github.com/latent-reasoning-works/manylatents-omics.git\n</code></pre></p>"},{"location":"extensions/#import-conflicts","title":"Import Conflicts","text":"<p>Problem: Namespace package not merging correctly.</p> <p>Solution: Ensure both packages have the namespace declaration in <code>manylatents/__init__.py</code>: <pre><code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n</code></pre></p>"},{"location":"extensions/#design-philosophy","title":"Design Philosophy","text":"<p>manyLatents is built around a simple idea: every interface between stages is a file with a known schema. This matters because the agents and scripts that compose manyLatents into larger workflows are stateless \u2014 they don't remember what happened in the last call. If the output of one step doesn't fully describe itself, the next step can't use it.</p> <p>This constraint shapes everything:</p> <ul> <li>LatentOutputs is a <code>dict[str, Any]</code>, not a dataclass. When a new metric injects a custom field, every downstream consumer still works without schema migration.</li> <li>Metrics are registered via Hydra configs with <code>_target_</code> and <code>_partial_: True</code>. Parameters are bound at config time, not at call time, so the evaluation engine doesn't need to know what parameters each metric takes.</li> <li>Algorithms are either <code>LatentModule</code> (fit/transform) or <code>LightningModule</code> subclasses (training loops). The execution engine dispatches on type, not on name.</li> </ul> <p>The result is a system where you can add a new algorithm, metric, dataset, or entire domain extension without touching core code.</p>"},{"location":"extensions/#two-execution-modes","title":"Two Execution Modes","text":"<p>CLI (<code>uv run python -m manylatents.main</code>) executes a single step: one algorithm + metrics on one dataset. This is the primary user-facing interface and what SLURM jobs invoke.</p> <p>Python API (<code>manylatents.api.run()</code>) is the programmatic interface designed for agent-driven workflows. It accepts <code>input_data</code> to chain the output of one call into the next, and supports <code>pipeline</code> configs for sequential steps within a single process.</p> <pre><code>from manylatents.api import run\n\n# Single step\nresult = run(\n    data='swissroll',\n    algorithms={'latent': {\n        '_target_': 'manylatents.algorithms.latent.pca.PCAModule',\n        'n_components': 50\n    }}\n)\n\n# Chaining: feed output of one step into the next\nresult2 = run(\n    input_data=result['embeddings'],\n    algorithms={'latent': {\n        '_target_': 'manylatents.algorithms.latent.phate.PHATEModule',\n        'n_components': 2\n    }}\n)\n</code></pre>"},{"location":"extensions/#namespace-extension-via-pkgutil","title":"Namespace Extension via pkgutil","text":"<p>The core package's <code>__init__.py</code> contains one line:</p> <pre><code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n</code></pre> <p>This tells Python: \"if another installed package also defines a <code>manylatents</code> directory, merge its contents into mine.\" The rule is simple: core never imports from extensions; extensions import from core.</p> <p>Extensions also register a Hydra <code>SearchPathPlugin</code> so their configs are discovered automatically:</p> <pre><code>class OmicsSearchPathPlugin(SearchPathPlugin):\n    def manipulate_search_path(self, search_path):\n        search_path.append(\n            provider=\"manylatents-omics\",\n            path=\"pkg://manylatents.dogma.configs\",\n        )\n</code></pre>"},{"location":"extensions/#four-extension-axes","title":"Four Extension Axes","text":""},{"location":"extensions/#1-algorithms","title":"1. Algorithms","text":"<p>Two base classes, binary decision rule:</p> <ul> <li><code>LatentModule</code> \u2014 fit/transform for non-neural algorithms (PCA, UMAP, PHATE, etc.). The FoundationEncoder pattern is a LatentModule where <code>fit()</code> is a no-op and <code>transform()</code> wraps a pretrained model.</li> <li><code>LightningModule</code> subclasses \u2014 trainable neural networks with Lightning training loops (autoencoders, Latent ODEs).</li> </ul> <p>Optional methods <code>kernel_matrix()</code> and <code>affinity_matrix()</code> enable module-level metrics like <code>KernelMatrixSparsity</code> and <code>AffinitySpectrum</code>.</p>"},{"location":"extensions/#2-metrics","title":"2. Metrics","text":"<p>Metrics follow the <code>Metric</code> protocol with three evaluation contexts:</p> Context <code>embeddings</code> <code>dataset</code> <code>module</code> Use case <code>embedding</code> Low-dim output Source dataset - Trustworthiness, continuity <code>dataset</code> - Source dataset - Stratification, admixture <code>module</code> - Source dataset Fitted LatentModule Affinity spectrum, kernel sparsity <p>List-valued parameters in configs expand via Cartesian product through <code>flatten_and_unroll_metrics()</code>. Metrics sharing kNN graphs use a shared <code>cache</code>.</p>"},{"location":"extensions/#3-data-modules","title":"3. Data Modules","text":"<p>Data modules provide <code>get_data()</code> and are auto-discovered at import time. Synthetic datasets generate on-the-fly; file-based datasets load from disk. For LightningModule algorithms, they also implement <code>LightningDataModule</code>.</p>"},{"location":"extensions/#4-domain-extensions","title":"4. Domain Extensions","text":"<p>A domain extension is a separate installable package that adds algorithms, metrics, and data modules to the <code>manylatents</code> namespace. See the Development tab for how to create one.</p>"},{"location":"extensions/#hydra-configuration","title":"Hydra Configuration","text":"<p>Every extensible component has a corresponding Hydra config group:</p> <pre><code>configs/\n  algorithms/\n    latent/         # LatentModule configs\n    lightning/      # LightningModule configs\n      loss/         # Loss function configs\n      network/      # Network architecture configs\n      optimizer/    # Optimizer configs\n  data/             # Dataset configs\n  metrics/\n    embedding/      # Embedding-level metric configs\n    dataset/        # Dataset-level metric configs\n    module/         # Module-level metric configs\n    sampling/       # Metric sampling strategies\n  callbacks/embedding/\n  experiment/       # Experiment preset configs\n  trainer/          # Lightning trainer configs\n  logger/           # Logger configs (none, wandb)\n  cluster/          # SLURM cluster configs (via Shop)\n  launcher/         # Job launcher configs (via Shop)\n</code></pre>"},{"location":"extensions/#scope-boundaries","title":"Scope Boundaries","text":"<p>manyLatents owns single-step execution and the Python API for composable workflows. It does NOT own:</p> <ul> <li>Multi-step orchestration \u2014 manyAgents calls <code>manylatents.api.run()</code> to compose steps</li> <li>RL / reward-driven training \u2014 Geomancer</li> <li>Cluster job submission \u2014 Shop provides Hydra launcher plugins</li> </ul>"},{"location":"extensions/#creating-an-extension","title":"Creating an Extension","text":"<p>This guide documents how to create extension packages for manyLatents, following the patterns established by <code>manylatents-omics</code>.</p>"},{"location":"extensions/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Your Application                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  manylatents-yourextension   \u2502   manylatents-omics          \u2502\n\u2502  (your namespace package)    \u2502   (popgen, dogma, singlecell)\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                        shop (optional)                       \u2502\n\u2502              (shared SLURM launchers, logging utils)         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                      manylatents (core)                      \u2502\n\u2502         (LatentModule, metrics, data, experiment runner)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"extensions/#package-structure","title":"Package Structure","text":"<pre><code>manylatents-yourextension/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 CLAUDE.md                    # AI assistant instructions\n\u251c\u2500\u2500 manylatents/\n\u2502   \u251c\u2500\u2500 __init__.py              # Namespace package declaration (CRITICAL)\n\u2502   \u251c\u2500\u2500 yourext_plugin.py        # Hydra SearchPathPlugin\n\u2502   \u2514\u2500\u2500 yourext/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 algorithms/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 your_algorithm.py\n\u2502       \u251c\u2500\u2500 data/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 your_dataset.py\n\u2502       \u251c\u2500\u2500 metrics/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 your_metric.py\n\u2502       \u2514\u2500\u2500 configs/\n\u2502           \u251c\u2500\u2500 __init__.py      # Empty, required for pkg://\n\u2502           \u251c\u2500\u2500 data/\n\u2502           \u2502   \u2514\u2500\u2500 your_data.yaml\n\u2502           \u251c\u2500\u2500 algorithms/\n\u2502           \u2502   \u2514\u2500\u2500 latent/\n\u2502           \u2502       \u2514\u2500\u2500 your_algo.yaml\n\u2502           \u251c\u2500\u2500 metrics/\n\u2502           \u2502   \u2514\u2500\u2500 dataset/\n\u2502           \u2502       \u2514\u2500\u2500 your_metric.yaml\n\u2502           \u2514\u2500\u2500 experiment/\n\u2502               \u2514\u2500\u2500 your_experiment.yaml\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_imports.py\n    \u2514\u2500\u2500 test_config_e2e.py\n</code></pre>"},{"location":"extensions/#critical-file-manylatents__init__py","title":"Critical File: <code>manylatents/__init__.py</code>","text":"<p>This file MUST contain the namespace package declaration:</p> <pre><code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n</code></pre> <p>Without this, Python won't merge your package with core manyLatents.</p>"},{"location":"extensions/#hydra-config-integration","title":"Hydra Config Integration","text":""},{"location":"extensions/#searchpathplugin","title":"SearchPathPlugin","text":"<p>Create <code>manylatents/yourext_plugin.py</code>:</p> <pre><code>from hydra.core.config_search_path import ConfigSearchPath\nfrom hydra.plugins.search_path_plugin import SearchPathPlugin\n\n\nclass YourExtSearchPathPlugin(SearchPathPlugin):\n    def manipulate_search_path(self, search_path: ConfigSearchPath) -&gt; None:\n        search_path.append(\n            provider=\"manylatents\",\n            path=\"pkg://manylatents.configs\",\n        )\n        # Higher priority for YOUR configs\n        search_path.prepend(\n            provider=\"manylatents-yourext\",\n            path=\"pkg://manylatents.yourext.configs\",\n        )\n</code></pre> <p>Use <code>prepend()</code> if your configs should override core configs with the same name, <code>append()</code> if core should take precedence.</p>"},{"location":"extensions/#entry-point-registration-recommended","title":"Entry Point Registration (Recommended)","text":"<p>Core auto-discovers extensions via the <code>manylatents.extensions</code> entry-point group. Add this to your <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"manylatents.extensions\"]\nyourext = \"manylatents.yourext_plugin:YourExtSearchPathPlugin\"\n</code></pre> <p>When your extension is installed (<code>uv pip install -e .</code>, <code>uv add</code>, or <code>pip install</code>), core's <code>manylatents.main</code> will automatically discover and register your plugin at startup. No custom entry point needed.</p>"},{"location":"extensions/#pyprojecttoml","title":"pyproject.toml","text":"<pre><code>[project]\nname = \"manylatents-yourext\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.10, &lt;3.13\"\n\ndependencies = [\n    \"manylatents\",\n]\n\n[project.entry-points.\"manylatents.extensions\"]\nyourext = \"manylatents.yourext_plugin:YourExtSearchPathPlugin\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"manylatents\"]  # CRITICAL: Package the manylatents/ directory\n\n[tool.uv]\nmanaged = true\n\n[tool.uv.sources]\nmanylatents = { git = \"https://github.com/latent-reasoning-works/manylatents.git\" }\n</code></pre>"},{"location":"extensions/#component-types","title":"Component Types","text":""},{"location":"extensions/#custom-latentmodule-algorithm","title":"Custom LatentModule (Algorithm)","text":"<pre><code>from torch import Tensor\nfrom manylatents.algorithms.latent.latent_module_base import LatentModule\n\n\nclass YourAlgorithm(LatentModule):\n    def __init__(self, n_components=2, your_param=1.0, **kwargs):\n        super().__init__(n_components=n_components, **kwargs)\n        self.your_param = your_param\n\n    def fit(self, x: Tensor) -&gt; None:\n        x_np = x.detach().cpu().numpy()\n        # Your fitting logic\n        self._is_fitted = True\n\n    def transform(self, x: Tensor) -&gt; Tensor:\n        if not self._is_fitted:\n            raise RuntimeError(\"Model not fitted. Call fit() first.\")\n        x_np = x.detach().cpu().numpy()\n        embedding = ...  # Your embedding computation\n        return torch.tensor(embedding, device=x.device, dtype=x.dtype)\n</code></pre> <p>Config: <code>manylatents/yourext/configs/algorithms/latent/your_algo.yaml</code></p> <pre><code>_target_: manylatents.yourext.algorithms.YourAlgorithm\nn_components: 2\nyour_param: 1.0\n</code></pre>"},{"location":"extensions/#custom-dataset","title":"Custom Dataset","text":"<pre><code>import numpy as np\n\n\nclass YourDataset:\n    def __init__(self, data_path: str, n_samples=None):\n        self.data_path = data_path\n        self._data = np.load(data_path)\n        if n_samples:\n            self._data = self._data[:n_samples]\n\n    def get_data(self) -&gt; np.ndarray:\n        return self._data\n\n    @property\n    def data(self) -&gt; np.ndarray:\n        return self._data\n</code></pre> <p>Config: <code>manylatents/yourext/configs/data/your_data.yaml</code></p> <pre><code>_target_: manylatents.yourext.data.YourDataset\ndata_path: ${paths.data_dir}/your_data.npy\nn_samples: null\n</code></pre>"},{"location":"extensions/#custom-metric","title":"Custom Metric","text":"<pre><code>import numpy as np\nfrom typing import Optional\nfrom manylatents.algorithms.latent.latent_module_base import LatentModule\n\n\ndef YourMetric(\n    embeddings: np.ndarray,\n    dataset: object,\n    module: Optional[LatentModule] = None,\n    threshold: float = 0.5,\n    return_per_sample: bool = False,\n) -&gt; float:\n    scores = ...  # Your metric computation\n    if return_per_sample:\n        return float(np.mean(scores)), scores\n    return float(np.mean(scores))\n</code></pre> <p>Config: <code>manylatents/yourext/configs/metrics/dataset/your_metric.yaml</code></p> <pre><code>_target_: manylatents.yourext.metrics.YourMetric\n_partial_: true  # CRITICAL: deferred parameter binding\nthreshold: 0.5\nreturn_per_sample: false\n</code></pre>"},{"location":"extensions/#experiment-config","title":"Experiment Config","text":"<p><code>manylatents/yourext/configs/experiment/your_experiment.yaml</code>:</p> <pre><code># @package _global_\nname: your_experiment\nproject: your_project\n\ndefaults:\n  - override /algorithms/latent: your_algo\n  - override /data: your_data\n  - override /callbacks/embedding: default\n  - override /metrics: default\n\nseed: 42\n</code></pre>"},{"location":"extensions/#ci-requirements","title":"CI Requirements","text":""},{"location":"extensions/#import-tests","title":"Import Tests","text":"<pre><code>def test_namespace_package():\n    import manylatents.yourext\n    from manylatents.yourext.algorithms import YourAlgorithm\n    from manylatents.data import SwissRoll  # Core still works\n    assert YourAlgorithm is not None\n\ndef test_algorithm_interface():\n    from manylatents.yourext.algorithms import YourAlgorithm\n    import torch\n\n    algo = YourAlgorithm(n_components=2)\n    X = torch.randn(100, 50)\n    embedding = algo.fit_transform(X)\n    assert embedding.shape == (100, 2)\n</code></pre>"},{"location":"extensions/#config-resolution-tests","title":"Config Resolution Tests","text":"<pre><code>from omegaconf import OmegaConf\nfrom pathlib import Path\n\nCONFIGS_DIR = Path(__file__).parent.parent / \"manylatents\" / \"yourext\" / \"configs\"\n\ndef test_all_targets_importable():\n    for config_file in CONFIGS_DIR.rglob(\"*.yaml\"):\n        cfg = OmegaConf.load(config_file)\n        if hasattr(cfg, \"_target_\"):\n            assert cfg._target_.startswith(\"manylatents\")\n</code></pre>"},{"location":"extensions/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code>name: CI\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n    steps:\n    - uses: actions/checkout@v4\n    - uses: astral-sh/setup-uv@v5\n    - run: uv sync\n    - run: uv run pytest tests/ -v\n    - run: |\n        uv run python -c \"\n        import manylatents\n        assert len(manylatents.__path__) &gt;= 2\n        print('Namespace package OK')\n        \"\n</code></pre>"},{"location":"extensions/#testing-checklist","title":"Testing Checklist","text":""},{"location":"extensions/#namespace-package","title":"Namespace Package","text":"<ul> <li> <code>manylatents/__init__.py</code> has <code>extend_path</code> line</li> <li> <code>import manylatents.yourext</code> works</li> <li> Core manylatents still importable</li> </ul>"},{"location":"extensions/#hydra-config","title":"Hydra Config","text":"<ul> <li> SearchPathPlugin registered via entry-point in <code>pyproject.toml</code></li> <li> Verify auto-discovery: <code>python -c \"from manylatents.extensions import discover_extensions; discover_extensions()\"</code></li> <li> All configs have valid <code>_target_</code> paths</li> <li> Metrics use <code>_partial_: true</code></li> <li> Experiment configs use <code># @package _global_</code></li> </ul>"},{"location":"extensions/#interface-compliance","title":"Interface Compliance","text":"<ul> <li> LatentModule subclasses implement <code>fit()</code> and <code>transform()</code></li> <li> Datasets have <code>get_data()</code> method</li> <li> Metrics accept <code>(embeddings, dataset, module, **kwargs)</code></li> </ul>"},{"location":"extensions/#ci","title":"CI","text":"<ul> <li> Import tests pass on Python 3.10-3.12</li> <li> Config resolution tests pass</li> <li> Core tests still pass with extension installed</li> </ul>"},{"location":"extensions/#quick-reference","title":"Quick Reference","text":"<pre><code># Core entry point with auto-discovery\nuv run python -m manylatents.main experiment=your_experiment\n</code></pre>"},{"location":"extensions/#troubleshooting-auto-discovery","title":"Troubleshooting Auto-Discovery","text":"<p>Check installed entry-points:</p> <pre><code>from importlib.metadata import entry_points\neps = entry_points(group=\"manylatents.extensions\")\nprint([(ep.name, ep.value) for ep in eps])\n</code></pre> <p>Verify plugin registration:</p> <pre><code>from manylatents.extensions import discover_extensions\ndiscover_extensions()\nfrom hydra.core.plugins import Plugins\nfrom hydra.plugins.search_path_plugin import SearchPathPlugin\nprint([p.__name__ for p in Plugins.instance().discover(SearchPathPlugin)])\n</code></pre> <p>Extension not showing up?</p> <ul> <li>Ensure the package is installed (not just cloned): <code>uv pip install -e .</code></li> <li>Check <code>pyproject.toml</code> has <code>[project.entry-points.\"manylatents.extensions\"]</code></li> <li>Reinstall after editing <code>pyproject.toml</code> \u2014 entry-points come from package metadata</li> </ul>"},{"location":"metrics/","title":"Metrics","text":"<p>The evaluation system for manyLatents: a three-level architecture for measuring embedding quality, dataset properties, and algorithm internals.</p>"},{"location":"metrics/#embedding-metrics","title":"Embedding Metrics","text":"<p>Evaluate the quality of low-dimensional embeddings. Compare high-dimensional input to low-dimensional output.</p> metric config defaults description AlignmentScore <code>metrics/embedding=alignment_score</code> k=20, method=jaccard Compute composite per-variant alignment score. Anisotropy <code>metrics/embedding=anisotropy</code> -- Anisotropy of embedding space AUC <code>metrics/embedding=auc</code> -- Area Under ROC Curve for binary classification CKA <code>metrics/embedding=cka</code> kernel=linear Centered Kernel Alignment with linear kernel Continuity <code>metrics/embedding=continuity</code> return_per_sample=True Continuity of embedding (preservation of original neighborhoods) CrossModalJaccard <code>metrics/embedding=cross_modal_jaccard</code> k=20, metric=euclidean Cross-modal k-NN neighborhood Jaccard overlap DiffusionCondensation <code>metrics/embedding=diffusion_condensation</code> scale=1.025, granularity=0.1, knn=5, decay=40, n_pca=50, output_mode=stable Diffusion condensation score DiffusionCurvature <code>metrics/embedding=diffusion_curvature</code> t=3, percentile=5 Diffusion curvature of embedding manifold DiffusionSpectralEntropy <code>metrics/embedding=diffusion_spectral_entropy</code> t=1, gaussian_kernel_sigma=10 Diffusion spectral entropy (eigenvalue count at diffusion time t) DiffusionSpectralEntropy <code>metrics/embedding=dse_dense</code> output_mode=eigenvalue_count, t_high=[10, 50, 100, 200, 500], numerical_floor=1e-06, kernel=dense, gaussian_kernel_sigma=10 Diffusion spectral entropy (eigenvalue count at diffusion time t) DiffusionSpectralEntropy <code>metrics/embedding=dse_knn</code> output_mode=eigenvalue_count, t_high=[10, 50, 100, 200, 500], numerical_floor=1e-06, kernel=knn, k=${neighborhood_size}, alpha=1.0 Diffusion spectral entropy (eigenvalue count at diffusion time t) DiffusionSpectralEntropy <code>metrics/embedding=dse_t_sweep</code> output_mode=eigenvalue_count_sweep, t_high=[10, 50, 100, 200, 500], numerical_floor=1e-06, gaussian_kernel_sigma=10 Diffusion spectral entropy (eigenvalue count at diffusion time t) FractalDimension <code>metrics/embedding=fractal_dimension</code> n_box_sizes=10 Correlation fractal dimension of embedding KNNPreservation <code>metrics/embedding=knn_preservation</code> n_neighbors=10, metric=euclidean k-nearest neighbor preservation between original and embedded spaces LocalIntrinsicDimensionality <code>metrics/embedding=local_intrinsic_dimensionality</code> k=20 Mean local intrinsic dimensionality of the embedding MagnitudeDimension <code>metrics/embedding=magnitude_dimension</code> n_ts=50, log_scale=False, scale_finding=convergence, target_prop=0.95, metric=euclidean, p=2, n_neighbors=12, method=cholesky, one_point_property=True, perturb_singularities=True, positive_magnitude=False, exact=False Magnitude-based effective dimensionality NoOp <code>metrics/embedding=noop</code> k=25 OutlierScore <code>metrics/embedding=outlier_score</code> k=20, return_scores=False Outlier scores using Local Outlier Factor ParticipationRatio <code>metrics/embedding=participation_ratio</code> n_neighbors=25, return_per_sample=True Local participation ratio measuring effective dimensionality PearsonCorrelation <code>metrics/embedding=pearson_correlation</code> return_per_sample=False, num_dists=100 Pearson correlation between pairwise distances PersistentHomology <code>metrics/embedding=persistent_homology</code> homology_dim=1, persistence_threshold=0.1 Count of loops/cycles (H1 Betti number) PersistentHomology <code>metrics/embedding=persistent_homology_beta0</code> homology_dim=0, persistence_threshold=3.0 Count of loops/cycles (H1 Betti number) RankAgreement <code>metrics/embedding=rank_agreement</code> k=20, metric_fn=lid Rank-based agreement of LID/PR across modalities ReebGraphNodesEdges <code>metrics/embedding=reeb_graph</code> n_bins=10 Reeb graph node and edge counts SilhouetteScore <code>metrics/embedding=silhouette</code> metric=euclidean Silhouette score for cluster separation in embedding TangentSpaceApproximation <code>metrics/embedding=tangent_space</code> n_neighbors=25, variance_threshold=0.95, return_per_sample=True Tangent space alignment between original and embedded spaces Trustworthiness <code>metrics/embedding=trustworthiness</code> n_neighbors=5, metric=euclidean Compute the trustworthiness of an embedding. Trustworthiness <code>metrics/embedding=trustworthiness_k</code> n_neighbors=[15, 25, 50, 100, 250], metric=euclidean Compute the trustworthiness of an embedding. <p>Config pattern: <code>metrics/embedding=&lt;name&gt;</code></p>"},{"location":"metrics/#module-metrics","title":"Module Metrics","text":"<p>Evaluate algorithm-specific internal components. Require a fitted module exposing <code>affinity_matrix()</code> or <code>kernel_matrix()</code>.</p> metric config defaults description AffinitySpectrum <code>metrics/module=affinity_spectrum</code> -- Top-k eigenvalues of the affinity matrix ConnectedComponents <code>metrics/module=connected_components</code> -- Number of connected components in the kNN graph DatasetTopologyDescriptor <code>metrics/module=dataset_topology_descriptor</code> -- Topological descriptor of the dataset structure DiffusionMapCorrelation <code>metrics/module=diffusion_map_correlation</code> dm_components=2, alpha=1.0, correlation_type=pearson Correlation between diffusion map and embedding distances KernelMatrixDensity <code>metrics/module=kernel_matrix_density</code> threshold=1e-10 Density of the kernel/affinity matrix KernelMatrixSparsity <code>metrics/module=kernel_matrix_sparsity</code> threshold=1e-10 Sparsity of the kernel/affinity matrix NoOp <code>metrics/module=noop</code> k=25 SpectralDecayRate <code>metrics/module=spectral_decay_rate</code> top_k=20 Fit exponential decay to the eigenvalue spectrum. SpectralGapRatio <code>metrics/module=spectral_gap_ratio</code> -- Ratio of first to second eigenvalue of the diffusion operator <p>Config pattern: <code>metrics/module=&lt;name&gt;</code></p>"},{"location":"metrics/#dataset-metrics","title":"Dataset Metrics","text":"<p>Evaluate properties of the original high-dimensional data, independent of the DR algorithm.</p> metric config defaults description GroundTruthPreservation <code>metrics/dataset=admixture_laplacian</code> -- Admixture Laplacian preservation score GeodesicDistanceCorrelation <code>metrics/dataset=geodesic_distance_correlation</code> correlation_type=spearman Correlation between geodesic and embedded distances NoOp <code>metrics/dataset=noop</code> k=25 kmeans_stratification <code>metrics/dataset=stratification</code> random_state=${seed} K-means stratification score for population structure <p>Config pattern: <code>metrics/dataset=&lt;name&gt;</code></p> ProtocolWriting a New MetricRunning Without Metrics"},{"location":"metrics/#metric-protocol","title":"Metric Protocol","text":"<p>All metrics must match the <code>Metric</code> protocol (<code>manylatents/metrics/metric.py</code>):</p> <pre><code>def __call__(\n    self,\n    embeddings: np.ndarray,\n    dataset=None,\n    module=None,\n    cache=None,\n) -&gt; float | tuple[float, np.ndarray] | dict[str, Any]\n</code></pre>"},{"location":"metrics/#return-types","title":"Return Types","text":"Type Use Case Example <code>float</code> Simple scalar Trustworthiness: <code>0.95</code> <code>tuple[float, ndarray]</code> Scalar + per-sample Continuity with <code>return_per_sample=True</code> <code>dict[str, Any]</code> Structured output Persistent homology: <code>{'beta_0': ..., 'beta_1': ...}</code>"},{"location":"metrics/#configuration","title":"Configuration","text":"<p>Metrics use Hydra's <code>_partial_: True</code> for deferred parameter binding:</p> <pre><code># configs/metrics/embedding/trustworthiness.yaml\n_target_: manylatents.metrics.trustworthiness.Trustworthiness\n_partial_: true\nn_neighbors: 5\n</code></pre>"},{"location":"metrics/#multi-scale-expansion","title":"Multi-Scale Expansion","text":"<p>List-valued parameters expand via Cartesian product through <code>flatten_and_unroll_metrics()</code>:</p> <pre><code>n_neighbors: [5, 10, 20]  # Produces 3 separate evaluations\n</code></pre> <p>Naming convention: <code>embedding.trustworthiness__n_neighbors_5</code>, <code>embedding.trustworthiness__n_neighbors_10</code>, etc.</p>"},{"location":"metrics/#shared-knn-cache","title":"Shared kNN Cache","text":"<p>Metrics that need kNN graphs share a cache computed once with <code>max(k)</code> across all metrics, avoiding redundant computation.</p>"},{"location":"metrics/#writing-a-new-metric","title":"Writing a New Metric","text":"<pre><code>import numpy as np\nfrom typing import Optional\n\ndef YourMetric(\n    embeddings: np.ndarray,\n    dataset=None,\n    module=None,\n    k: int = 10,\n    cache=None,\n) -&gt; float:\n    # Your computation\n    return score\n</code></pre>"},{"location":"metrics/#choosing-the-right-level","title":"Choosing the Right Level","text":"<ul> <li>Only needs original data? \u2192 <code>metrics/dataset/</code></li> <li>Compares original vs. reduced? \u2192 <code>metrics/embedding/</code></li> <li>Needs algorithm internals? \u2192 <code>metrics/module/</code></li> </ul>"},{"location":"metrics/#config","title":"Config","text":"<pre><code># configs/metrics/embedding/your_metric.yaml\n_target_: manylatents.metrics.your_metric.YourMetric\n_partial_: true\nk: 10\n</code></pre>"},{"location":"metrics/#testing","title":"Testing","text":"<p>Use <code>metrics=noop</code> to verify integration:</p> <pre><code>uv run python -m manylatents.main data=swissroll algorithms/latent=pca metrics=noop\n</code></pre>"},{"location":"metrics/#null-metrics-support","title":"Null Metrics Support","text":"<p>manyLatents supports running experiments without metrics computation \u2014 useful for fast debugging, exploratory analysis, or workflows where metrics are computed separately.</p>"},{"location":"metrics/#usage","title":"Usage","text":""},{"location":"metrics/#cli-default","title":"CLI (Default)","text":"<p>Metrics are null by default. Just don't specify them:</p> <pre><code># No metrics (default)\nuv run python -m manylatents.main data=swissroll algorithms/latent=pca\n\n# With metrics (explicit opt-in)\nuv run python -m manylatents.main data=swissroll algorithms/latent=pca metrics=noop\n</code></pre>"},{"location":"metrics/#experiment-configs","title":"Experiment Configs","text":"<pre><code># configs/experiment/my_experiment.yaml\n# @package _global_\ndefaults:\n  - override /algorithms/latent: pca\n  - override /data: swissroll\n  - override /callbacks/embedding: default\n  # No metrics override - stays null\n</code></pre>"},{"location":"metrics/#python-api","title":"Python API","text":"<pre><code>from manylatents.api import run\n\nresult = run(\n    data=\"swissroll\",\n    algorithms={'latent': 'pca'},\n    metrics=None  # Explicitly disable\n)\n</code></pre>"},{"location":"metrics/#expected-behavior","title":"Expected Behavior","text":"<p>When <code>metrics=null</code>:</p> <ul> <li>Generates embeddings</li> <li>Saves embeddings to files</li> <li>Creates plots (if callbacks configured)</li> <li>Logs to wandb (if configured)</li> <li>Does NOT compute evaluation metrics</li> <li>Shows warning: \"No scores found\"</li> </ul>"},{"location":"metrics/#design-opt-in-by-default","title":"Design: Opt-In by Default","text":"<p>The base config (<code>configs/config.yaml</code>) sets metrics to <code>null</code>. Experiment configs opt in:</p> <pre><code># configs/experiment/single_algorithm.yaml\ndefaults:\n  - override /metrics: noop  # Opt in for this experiment\n</code></pre>"},{"location":"metrics/#hydra-limitation","title":"Hydra Limitation","text":"<p>Hydra CLI does not support <code>null</code> as an override value. You cannot do <code>metrics=null</code> on the command line \u2014 Hydra's parser converts <code>\"null\"</code> to Python <code>None</code>, which its override validator rejects.</p> <p>Workarounds:</p> <ul> <li>Use experiment configs without metrics specified</li> <li>Use the Python API with <code>metrics=None</code> (our code handles this)</li> <li>Use <code>metrics=null</code> config files (e.g., the base config already does this)</li> </ul> <p>The API intercepts <code>None</code> values before Hydra sees them and sets them after config composition via <code>OmegaConf.update()</code>.</p>"},{"location":"metrics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metrics/#could-not-find-metricsnone","title":"\"Could not find 'metrics/none'\"","text":"<p>You're trying <code>metrics=none</code> as a CLI override. Hydra interprets this as looking for <code>metrics/none.yaml</code>.</p> <p>Fix: Use an experiment config, or the API with <code>metrics=None</code>.</p>"},{"location":"metrics/#metrics-still-being-computed","title":"Metrics Still Being Computed","text":"<p>Check that:</p> <ol> <li>Your experiment config doesn't have <code>- override /metrics: ...</code> in defaults</li> <li>You're not passing <code>metrics=...</code> on the command line</li> <li>The final config shows <code>metrics: null</code></li> </ol>"},{"location":"probing/","title":"Representation Probing Guide","text":"<p>This guide explains how to probe neural network representations during training using the manylatents infrastructure.</p>"},{"location":"probing/#overview","title":"Overview","text":"<p>Representation probing extracts activations from model layers at configurable points during training and computes diffusion operators to track how the representation geometry evolves.</p> <p>Key components: - <code>LayerSpec</code> - Specifies which layer to probe - <code>ActivationExtractor</code> - Hooks into the model to capture activations - <code>DiffusionGauge</code> - Converts activations to diffusion operators - <code>ActivationTrajectoryCallback</code> - Orchestrates probing during Lightning training</p>"},{"location":"probing/#quick-start","title":"Quick Start","text":"<p>Run the default probing experiment:</p> <pre><code>uv run python -m manylatents.main experiment=representation_probe \\\n    algorithms.lightning.config.model_name_or_path=gpt2 \\\n    trainer.max_epochs=3 \\\n    logger=wandb\n</code></pre> <p>This trains GPT-2 on WikiText-2 while probing the last transformer layer every 500 steps.</p>"},{"location":"probing/#configuration","title":"Configuration","text":""},{"location":"probing/#layer-specification","title":"Layer Specification","text":"<p><code>LayerSpec</code> defines where to extract activations:</p> <pre><code>layer_specs:\n  - path: \"transformer.h[-1]\"      # Layer path (supports indexing)\n    extraction_point: \"output\"     # What to capture\n    reduce: \"mean\"                 # How to reduce sequence dimension\n</code></pre> <p>Path syntax: - <code>transformer.h[-1]</code> - Last transformer block - <code>transformer.h[0]</code> - First transformer block - <code>encoder.layer.5</code> - Specific layer by index - <code>model.embed_tokens</code> - Embedding layer</p> <p>Extraction points: - <code>output</code> - Layer output activations - <code>input</code> - Layer input activations</p> <p>Reduction methods: - <code>mean</code> - Average over sequence length \u2192 <code>[batch, hidden_dim]</code> - <code>last</code> - Last token only \u2192 <code>[batch, hidden_dim]</code> - <code>first</code> - First token (e.g., CLS) \u2192 <code>[batch, hidden_dim]</code> - <code>none</code> - Keep full sequence \u2192 <code>[batch, seq_len, hidden_dim]</code></p>"},{"location":"probing/#probe-triggers","title":"Probe Triggers","text":"<p>Control when probing occurs:</p> <pre><code>trigger:\n  every_n_steps: 500        # Probe every N training steps\n  every_n_epochs: null      # Or every N epochs\n  on_checkpoint: true       # Probe when checkpointing\n  on_validation_end: true   # Probe after validation\n</code></pre> <p>Multiple triggers combine with OR logic.</p>"},{"location":"probing/#diffusion-gauge","title":"Diffusion Gauge","text":"<p>Configure how activations become diffusion operators:</p> <pre><code>gauge:\n  knn: 15          # k for k-NN graph\n  alpha: 1.0       # Gaussian kernel bandwidth\n  symmetric: false # Row-stochastic (false) or symmetric normalization\n</code></pre> <p>The gauge computes: activations \u2192 k-NN graph \u2192 Gaussian kernel \u2192 diffusion operator</p>"},{"location":"probing/#multi-layer-probing","title":"Multi-Layer Probing","text":"<p>Probe multiple layers simultaneously:</p> <pre><code>callbacks:\n  trainer:\n    probe:\n      layer_specs:\n        - path: \"transformer.h[0]\"\n          extraction_point: \"output\"\n          reduce: \"mean\"\n        - path: \"transformer.h[5]\"\n          extraction_point: \"output\"\n          reduce: \"mean\"\n        - path: \"transformer.h[-1]\"\n          extraction_point: \"output\"\n          reduce: \"mean\"\n</code></pre>"},{"location":"probing/#probe-dataloader","title":"Probe Dataloader","text":"<p>Probing uses a fixed subset of data for consistent comparisons across training. The <code>TextDataModule</code> provides this via <code>probe_dataloader()</code>:</p> <pre><code>data:\n  probe_n_samples: 512  # Size of probe subset\n  seed: 42              # Reproducible subset selection\n</code></pre>"},{"location":"probing/#wandb-logging","title":"WandB Logging","text":"<p>Enable trajectory logging to WandB:</p> <pre><code>callbacks:\n  trainer:\n    probe:\n      log_to_wandb: true\n\nlogger: wandb\n</code></pre> <p>This logs: - Diffusion operator eigenspectra - Trajectory visualizations (if configured) - Step-indexed operator snapshots</p>"},{"location":"probing/#programmatic-access","title":"Programmatic Access","text":"<p>Access probe results after training:</p> <pre><code>from manylatents.lightning.callbacks.activation_tracker import ActivationTrajectoryCallback\n\n# After trainer.fit()\ncallback = trainer.callbacks[0]  # Get probe callback\ntrajectory = callback.get_trajectory()\n\nfor step, diffusion_op in trajectory:\n    print(f\"Step {step}: operator shape {diffusion_op.shape}\")\n</code></pre>"},{"location":"probing/#custom-probes","title":"Custom Probes","text":"<p>Extend <code>DiffusionGauge</code> for custom probe computations:</p> <pre><code>from manylatents.callbacks.diffusion_operator import DiffusionGauge\nimport numpy as np\n\nclass MyGauge(DiffusionGauge):\n    def __call__(self, activations: np.ndarray) -&gt; np.ndarray:\n        # Custom computation\n        P = super().__call__(activations)  # Get diffusion operator\n        # Add your analysis...\n        return P\n</code></pre>"},{"location":"probing/#slurm-sweeps","title":"SLURM Sweeps","text":"<p>Run probing sweeps on cluster:</p> <pre><code>uv run python -m manylatents.main -m \\\n    experiment=representation_probe \\\n    cluster=mila \\\n    algorithms.lightning.config.model_name_or_path=gpt2,gpt2-medium \\\n    callbacks.trainer.probe.gauge.knn=10,15,25\n</code></pre> <p>See <code>configs/experiment/representation_probe.yaml</code> for a full experiment example.</p>"},{"location":"probing/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  HFTrainerModule \u2502 \u2190 Any HuggingFace model\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 forward hooks\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502ActivationExtractor\u2502 \u2190 Captures layer outputs\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 activations [batch, hidden_dim]\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  DiffusionGauge  \u2502 \u2190 k-NN \u2192 kernel \u2192 operator\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 diffusion operator [n_samples, n_samples]\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 WandbProbeLogger \u2502 \u2190 Logs to W&amp;B\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"probing/#files","title":"Files","text":"File Description <code>lightning/hooks.py</code> LayerSpec, ActivationExtractor <code>lightning/callbacks/activation_tracker.py</code> ActivationTrajectoryCallback, ProbeTrigger <code>callbacks/diffusion_operator.py</code> DiffusionGauge, build_diffusion_operator <code>lightning/callbacks/wandb_probe.py</code> WandB logging <code>configs/callbacks/trainer/probe.yaml</code> Default probe config <code>configs/experiment/representation_probe.yaml</code> Full experiment config"},{"location":"testing/","title":"Testing","text":"<p>Testing strategy and CI pipeline for manyLatents.</p> CI PipelineNamespace Extensions"},{"location":"testing/#github-actions-pipeline","title":"GitHub Actions Pipeline","text":""},{"location":"testing/#two-phase-testing","title":"Two-Phase Testing","text":"<ol> <li>Build &amp; Unit Tests (20 min): Unit tests + CLI smoke tests for both LatentModule and LightningModule paths</li> <li>Discovery Smoke Tests (conditional): Automatic testing of all algorithm/metric configs when their source changes</li> </ol>"},{"location":"testing/#discovery-smoke-tests","title":"Discovery Smoke Tests","text":"<p>Triggers on changes to:</p> <ul> <li><code>manylatents/algorithms/latent/**</code> \u2192 runs <code>.github/workflows/scripts/test_latent_algorithms.sh</code></li> <li><code>manylatents/algorithms/lightning/**</code> \u2192 runs <code>.github/workflows/scripts/test_lightning_algorithms.sh</code></li> <li><code>manylatents/metrics/**</code> \u2192 runs <code>.github/workflows/scripts/test_metrics.sh</code></li> </ul> <p>Each script dynamically discovers all configs in its directory and runs quick instantiation tests using minimal test data.</p> <p>Validates:</p> <ul> <li>Algorithm/metric config is valid YAML</li> <li>Target class can be instantiated via Hydra</li> <li>Full pipeline completes without errors</li> </ul> <p>Does NOT test: mathematical correctness (unit tests cover that).</p>"},{"location":"testing/#cli-smoke-tests","title":"CLI Smoke Tests","text":"<p>Every CI run includes two fixed smoke tests:</p> Test Config Purpose LatentModule CLI <code>experiment=single_algorithm metrics=noop</code> Validates non-neural path LightningModule CLI <code>algorithms/lightning=ae_reconstruction trainer.fast_dev_run=true</code> Validates neural path"},{"location":"testing/#local-testing","title":"Local Testing","text":"<pre><code># Quick smoke test (LatentModule)\nuv run python -m manylatents.main \\\n  algorithms/latent=noop data=test_data metrics=noop logger=none\n\n# PCA + SwissRoll with metrics\nuv run python -m manylatents.main \\\n  algorithms/latent=pca data=swissroll \\\n  metrics/embedding=trustworthiness logger=none\n\n# Autoencoder + SwissRoll (fast dev run)\nuv run python -m manylatents.main \\\n  algorithms/lightning=ae_reconstruction data=swissroll \\\n  trainer=default trainer.max_epochs=2 trainer.fast_dev_run=true \\\n  logger=none\n\n# Full pytest suite\nuv run pytest tests/ -x -q\n\n# Callback tests\nuv run pytest manylatents/callbacks/tests/ -x -q\n\n# Docs coverage check\nuv run python scripts/check_docs_coverage.py\n</code></pre>"},{"location":"testing/#adding-new-algorithm-tests","title":"Adding New Algorithm Tests","text":"<p>Discovery scripts auto-detect new configs. Just add a YAML file to the right directory:</p> <ul> <li><code>manylatents/configs/algorithms/latent/your_algo.yaml</code> \u2014 auto-discovered by <code>test_latent_algorithms.sh</code></li> <li><code>manylatents/configs/algorithms/lightning/your_algo.yaml</code> \u2014 auto-discovered by <code>test_lightning_algorithms.sh</code></li> <li><code>manylatents/configs/metrics/embedding/your_metric.yaml</code> \u2014 auto-discovered by <code>test_metrics.sh</code></li> </ul>"},{"location":"testing/#failure-investigation","title":"Failure Investigation","text":"<ol> <li>Check test-specific artifacts in GitHub Actions</li> <li>Run the same configuration locally</li> <li>Enable <code>HYDRA_FULL_ERROR=1</code> for detailed stack traces</li> </ol>"},{"location":"testing/#testing-manylatents-extension-packages","title":"Testing manylatents + Extension Packages","text":"<p>manyLatents uses <code>pkgutil.extend_path()</code> for namespace extensions. The <code>manylatents-omics</code> package adds:</p> <ul> <li><code>manylatents.dogma</code> \u2014 foundation model encoders (Evo2, ESM3, Orthrus, AlphaGenome)</li> <li><code>manylatents.popgen</code> \u2014 population genetics datasets and metrics</li> <li><code>manylatents.singlecell</code> \u2014 AnnData integration</li> </ul>"},{"location":"testing/#verification","title":"Verification","text":""},{"location":"testing/#package-discovery","title":"Package Discovery","text":"<pre><code>import manylatents\nprint(manylatents.__path__)  # Should show multiple paths if extension installed\n</code></pre>"},{"location":"testing/#module-loading","title":"Module Loading","text":"<pre><code># Only available with manylatents-omics installed\nfrom manylatents.dogma.encoders import Evo2Encoder\n</code></pre>"},{"location":"testing/#key-principle","title":"Key Principle","text":"<p>Core <code>manylatents</code> never imports from extensions. Extensions register configs via Hydra's <code>SearchPathPlugin</code> and are discovered at runtime.</p>"},{"location":"testing/#namespace-declaration","title":"Namespace Declaration","text":"<p>Both packages must have in <code>manylatents/__init__.py</code>:</p> <pre><code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n</code></pre>"},{"location":"testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing/#no-module-named-manylatentsdogma","title":"\"No module named 'manylatents.dogma'\"","text":"<p>Install the extension package:</p> <pre><code>pip install manylatents-omics\n# or for development\nuv add -e /path/to/manylatents-omics --no-deps\n</code></pre>"},{"location":"testing/#cli-breaks-with-extension","title":"CLI Breaks With Extension","text":"<p>The <code>manylatents.dogma</code> namespace is imported conditionally. If the extension has unmet dependencies, the CLI may fail. Install with all extras: <code>pip install manylatents-omics[all]</code>.</p>"}]}